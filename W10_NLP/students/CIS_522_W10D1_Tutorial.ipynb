{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchtext==0.15.1 matplotlib numpy datatops==0.2.2 portalocker==2.7.0 vibecheck==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/rqe7P2A1gL4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10567d1c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('rqe7P2A1gL4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
    "my_pennkey = \"\"  # @param {type:\"string\"}\n",
    "my_pod = \"\"  # @param {type:\"string\"}\n",
    "my_email = \"\"  # @param {type:\"string\"}\n",
    "tutorial = \"W10D1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback setup (run this cell)\n",
    "\n",
    "# Feedback with Datatops\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from datatops import Datatops\n",
    "\n",
    "feedback_dtid = \"62a48t3w\"\n",
    "feedback_name = \"cis522_feedback\"\n",
    "quiz_dtid = \"lxx8szk1\"\n",
    "quiz_name = \"cis522_quiz\"\n",
    "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
    "\n",
    "# Instantiate the Datatops client\n",
    "dt = Datatops(dt_url)\n",
    "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(522)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/ELRVHvQdXHs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10567d250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('ELRVHvQdXHs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Attention\n",
    "\n",
    "Last week, we discussed natural language processing and the state of the art circa the early 2010s. In this tutorial, we will discuss the attention mechanism and how it can be used to improve the performance of a neural network, and we will implement attention and Transformer models.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Previously, we discussed an encoder-decoder architecture based on two RNNs for sequence to sequence learning. Specifically, the RNN encoder transforms a variable-length sequence into a fixed-shape context variable, then the RNN decoder generates the output (target) sequence token by token based on the generated tokens and the context variable. However, even though not all the input (source) tokens are useful for decoding a certain token, the same context variable that encodes the entire input sequence is still used at each decoding step. It is challenging for the models to deal with long sentences. \n",
    "\n",
    "In [Bahdanau et al., 2014], the authors proposed a technique called attention. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are relevant to the current prediction. \n",
    "\n",
    "<img width=\"733\" alt=\"image\" src=\"https://user-images.githubusercontent.com/693511/226738862-12a6199e-f204-4e9b-8af5-2a23a946802c.png\">\n",
    "\n",
    "<small>An example of attention matrix. Taken from [arxiv.1409.0473]. Don't worry too much about how we derived this just yet!</small>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a: Computing attention \n",
    "\n",
    "To calculate the attention mechanism we make use of Queries, Keys and Values. These tensors are each a transformation of the input vector:\n",
    "\n",
    "In an attention mechanism the context vector is computed as a weighted sum of values, where the weight assigned to each value is computed through an attention score. The score is usually the dot product between the query and key. The scores then go through the softmax function to yield a set of weights whose sum equals 1 — in other words, the weights are normalized so that every target token is assigned a \"how much does this token matter\" value for each output token.\n",
    "\n",
    "The query is from the decoder hidden state whereas the key and value are from the encoder hidden state. \n",
    "\n",
    "Take a minute and look at this article. It has detailed graphical explanation on how to calculate attention scores. \n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "Attention mechanisms in machine learning have three components:\n",
    "\n",
    " - the query Q (an embedding of the target word)\n",
    " - the keys K (an embedding of the source words)\n",
    " - the values V (a function of K... you can think of these as the database lookup values that correspond to K)\n",
    " \n",
    "Generally the number and dimensionality of queries and values can all be different.\n",
    "\n",
    "### The attention algorithm\n",
    "\n",
    "To give a concrete example, let's imagine a simple, single-sentence document. When I look at a sentence and see a word $q$, I want to know which word $k$ in the document is most relevant to understand what $q$ means. In other words, given a \"database\" of key-value pairs, I want to find the [set of] keys that best match my query.\n",
    "\n",
    "We'll start by constructing a query vector space $Q = $X \\dot W_Q^T$ where $X$ is the input sequence (i.e., our starting sentence that has been tokenized and embedded) and $W_Q$ is a matrix of weights that we'll learn. \n",
    "\n",
    "Next, we'll think of each word in the sentence as keys that we can use to \"look up\" the answer to our query; mathematically, we'll do the same as the above for the keys $K = X \\dot W_K^T$, and the value tensor $V = X \\dot W_V^T$.\n",
    "\n",
    "For each pair of words $(q, k)$, we'll compute a score $s(q, k)$ that tells us how relevant $k$ is to $q$. We'll do this by taking the dot product of the query and key vectors, $Q \\cdot K^T$.\n",
    "\n",
    "Finally, we can compute the final output of our attention head; $A = softmax(s(Q, K)) \\cdot V$. This is the weighted sum of the values, where the weights are the attention scores.\n",
    "\n",
    "\n",
    "We don't know ahead of time what $Q$, $K$, $V$ should be; we'll learn the weights from the data via backpropagation.\n",
    "\n",
    "\n",
    "In general, we'll use existing packages to implement attention. However, it's important to understand the code behind the magic, so we'll implement a simple attention mechanism in the next section using PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz!\n",
    "\n",
    "Are you still with us? This was a complicated concept! Test your knowledge and intuitions about the algorithm so far. Feel free to play with the code below and come back to these questions after manipulating the attention block!\n",
    "\n",
    "**We compute `embed_K(\"dog\")` and `embed_Q(\"dog\")`, and then take the dot-product. If you take the dot product of two identical vectors, you should get out 1.0. Why don't we get 1.0 when we take the dot product of these two embeddings of the same word?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_k_embed_q_embed = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D1_Attention\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this if above link is broken\n",
    "\n",
    "url = \"https://jalammar.github.io/images/seq2seq_7.mp4\"\n",
    "from IPython.display import HTML\n",
    "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the IMDB sentiment analysis dataset using PyTorch + torchtext\n",
    "# https://pytorch.org/text/stable/datasets.html#imdb\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_BATCH_SIZE = 32\n",
    "train_data, test_data = IMDB(split=(\"train\", \"test\"))\n",
    "\n",
    "# Create a vocabulary from the training data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(\n",
    "    map(lambda x: tokenizer(x[1]), train_data), specials=[\"<unk>\", \"<pad>\"]\n",
    ")\n",
    "\n",
    "# Add the labels to the vocabulary\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create the data loaders\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def generate_batch(batch, sentence_length=_BATCH_SIZE):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor([vocab[token] for token in tokenizer(_text)])\n",
    "        processed_text = torch.cat(\n",
    "            [\n",
    "                processed_text,\n",
    "                torch.tensor(\n",
    "                    [vocab[\"<pad>\"]] * (sentence_length - len(processed_text))\n",
    "                ),\n",
    "            ]\n",
    "        ).long()\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list)\n",
    "    # Pad the text sequences, guarantee that all sequences are of the same length\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    # Truncate the text sequences\n",
    "    text_list = text_list[:, :sentence_length]\n",
    "    return (label_list, text_list)\n",
    "\n",
    "\n",
    "train_iter = DataLoader(\n",
    "    to_map_style_dataset(train_data),\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=generate_batch,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    to_map_style_dataset(test_data),\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=generate_batch,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Attn doesn't preserve sequence. It only counts importance, not the relation or order. And order matters in language!\n",
    "\n",
    "Using varying frequencies of sine and cosine functions of different frequencies, we can create a vector that encodes the relative position of a token in a sequence that lets the neural network decide for itself which frequencies it cares about alongside the other features it's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Video : Positional Encoding\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"FoRWkEAJDtg\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "\n",
    "import time\n",
    "try: t0;\n",
    "except NameError: t0=time.time()\n",
    "\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.position_embedding = torch.zeros((1, max_len, embed_dim))\n",
    "        i = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)\n",
    "        j2 = torch.arange(0, embed_dim, step=2, dtype=torch.float32)\n",
    "        x = i / torch.pow(10000, j2 / embed_dim)\n",
    "        self.position_embedding[..., 0::2] = torch.sin(x)\n",
    "        self.position_embedding[..., 1::2] = torch.cos(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_plus_p = x + self.position_embedding[:, : x.shape[1]]\n",
    "        return x_plus_p\n",
    "\n",
    "\n",
    "\n",
    "n_tokens, embed_dim = 40, 40\n",
    "pos_enc = PositionalEncoder(embed_dim)\n",
    "p = pos_enc(torch.zeros((1, n_tokens, embed_dim)))\n",
    "plt.imshow(p.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D1_PositionalEncoding\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Transformers\n",
    "\n",
    "Take a deep breath. It is finally time to apply everything we have learned in the last three notebooks — it's Transformers time!\n",
    "\n",
    "Like CNNs, the term \"Transformer\" refers not to a specific architecture, but a _style_ of architecture. And this base model architecture has been the foundation of many very successful models, some of which you may be using right now to write your answers to this notebook, and others of which you might be asking to make you reservations at a restaurant tonight. It has seen tremendous use in the NLP space, and there have been effforts to extend it to the audio and visual domains as well. The original paper that explained the Transformer architecture had the rather aggressive title, \"Attention is all you need\" — but so far, it has proven to be more correct than many expected! \n",
    "\n",
    "[Attention Is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762), is very readable and you are encouraged to take a look.\n",
    "\n",
    "The Transformer model is fundamentally an encoder-decoder model that operates on sequences of tokens. Both the encoder and decoder components are composed of stacks of submodules that use **only** attention mechanisms and linear weights to learn (there are no CNNs or RNNs). The architecture schematic looks like the following:\n",
    "\n",
    "![transformer architecture](https://d2l.ai/_images/transformer.svg)\n",
    "\n",
    "Let's break this down, because while the architecture can seem very daunting, you now know all the pieces that go into it! \n",
    "\n",
    "* An encoder/decoder network is familiar territory; we've written autoencoders before, and used this style of model extensively for image generation in a previous week.\n",
    "* Positional encoding: We wrote this from scratch just now!\n",
    "* Sequences of tokens? We not only know what that looks like — we've written our own tokenizers!\n",
    "* Attention is the trickiest piece, but we've implemented it from scratch above. Nothing scary here!\n",
    "\n",
    "In the rest of this section we will be going over the various building blocks that go into Transformers. The emphasis is on understanding what all the pieces do and how they fit together, so while we _will_ train a transformer in this notebook, you should not expect it to write your homework any time soon.\n",
    "\n",
    "*Note:* Many of the images in this section are taken from Dive Into Deep Learning's chapter on [Attention Mechanisms](https://d2l.ai/chapter_attention-mechanisms/index.html). You are encouraged to check that out for additional details and implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Adh2ldd1oIQ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "Transformers make use of something called self-attention as a critical component to the entire operation. What does that mean in the context of attention mechanisms? If you recall, attention mechanisms in machine learning have three components:\n",
    "\n",
    " - the values V (the things you perceive i.e. model inputs)\n",
    " - the query Q (the thing you want to attend to)\n",
    " - the keys K (a mapping between queries and values)\n",
    " \n",
    "Generally the number and dimensionality of queries and values can all be different. In self-attention, the queries, keys, and values are all drawn from the same set of inputs. In other words, we don't need to specify anything about what and how queries and keys are formed, as they come straight from the data just like the values!\n",
    "\n",
    "Take a minute and check out the following article; It has detailed graphical explanation on how to calculate attention scores \n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "Ok, so we know that our queries, keys, and values come from our input sequence, but which attention mechanism should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masked Scaled Dot Product Attention\n",
    "#@title Video : Masking\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"VtaGIp_9j1w\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Module\n",
    "Next we will show you how to implement the forward method of a PyTorch module for handling the multi-head attention mechanism. Each of the Q, K, and V inputs need to be run through their corresponding linear layers and then transformed using `mha_transform_input`. You then pass these to our scaled dot product attention module, transform that output back using `mha_transform_output`, and then run that though the corresponding output linear layer.\n",
    "\n",
    "*NOTE:* In the original Transformers paper, the linear layers were just weight matrices with no bias term which is reproduced here by using `Linear` layers and setting bias to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(x, mask):\n",
    "    \"\"\"Applies softmax on a masked version of the input.\n",
    "    Args:\n",
    "      x (n_batch, n_tokens, t_tokens): - the scaled dot product of Q and K\n",
    "      mask (n_batch, n_tokens): - binary mask, all values = 0 will be set to -inf\n",
    "    Returns:\n",
    "      (n_batch, n_tokens, n_tokens): the result of applying softmax along the last\n",
    "        dimension of the masked input.\n",
    "    \"\"\"\n",
    "    return F.softmax(x.masked_fill_(mask.unsqueeze(1) == 0, float(\"-inf\")), dim=-1)\n",
    "\n",
    "\n",
    "def mha_transform_input(x, n_heads, head_dim):\n",
    "    \"\"\"Restructure the input tensors to compute the heads in parallel\n",
    "    Requires that head_dim = embed_dim / n_heads\n",
    "    Args:\n",
    "      x (n_batch, n_tokens, embed_dim): input tensor, one of queries, keys, or values\n",
    "      n_heads (int): the number of attention heads\n",
    "      head_dim (int): the dimensionality of each head\n",
    "    Returns:\n",
    "      (n_batch*n_heads, n_tokens, head_dim): 3D Tensor containing all the input heads\n",
    "    \"\"\"\n",
    "    n_batch, n_tokens, _ = x.shape\n",
    "    x = x.reshape((n_batch, n_tokens, n_heads, head_dim))\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    return x.reshape((n_batch * n_heads, n_tokens, head_dim))\n",
    "\n",
    "\n",
    "def mha_transform_output(x, n_heads, head_dim):\n",
    "    \"\"\"Restructures the output back to the original format\n",
    "    Args:\n",
    "      x (n_bacth*n_heads, n_tokens, head_dim): multi-head representation tensor\n",
    "      n_heads (int): the number of attention heads\n",
    "      head_dim (int): the dimensionality of each head\n",
    "    Returns:\n",
    "      (n_batch, n_tokens, embed_dim): 3D Tensor containing all the input heads\n",
    "    \"\"\"\n",
    "    n_concat, n_tokens, _ = x.shape\n",
    "    n_batch = n_concat // n_heads\n",
    "    x = x.reshape((n_batch, n_heads, n_tokens, head_dim))\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    return x.reshape((n_batch, n_tokens, n_heads * head_dim))\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
    "          keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
    "          values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
    "          mask (n_batch, n_tokens): binary mask tensor\n",
    "        Returns:\n",
    "          (n_batch, n_tokens, embed_dim): scaled dot product attention tensor\n",
    "        \"\"\"\n",
    "        scaled_dot_product = torch.bmm(queries, torch.transpose(keys, 1, 2)) / np.sqrt(\n",
    "            self.embed_dim\n",
    "        )\n",
    "        masked_softmax_scores = masked_softmax(scaled_dot_product, mask)\n",
    "        attention = torch.bmm(masked_softmax_scores, values)\n",
    "        return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(embed_dim)\n",
    "        self.query_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
    "          keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
    "          values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
    "          mask (n_batch, n_tokens): binary mask tensor\n",
    "        Returns:\n",
    "          (n_batch, n_tokens, embed_dim): multi-head attention tensor\n",
    "        \"\"\"\n",
    "        q_heads = mha_transform_input(\n",
    "            self.query_fc(queries), self.n_heads, self.head_dim\n",
    "        )\n",
    "        k_heads = mha_transform_input(self.key_fc(keys), self.n_heads, self.head_dim)\n",
    "        v_heads = mha_transform_input(\n",
    "            self.value_fc(values), self.n_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        attention_heads = self.attention(q_heads, k_heads, v_heads, mask)\n",
    "        attention = self.out_fc(\n",
    "            mha_transform_output(attention_heads, self.n_heads, self.head_dim)\n",
    "        )\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Video : Transformer Architecture\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"_sKZpAptIZk\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture: Encoder\n",
    "\n",
    "We now have almost everything we need to assemble the full Transformer network. There are just two more modules we need to quickly discuss, and then we will get to putting them all together.\n",
    "\n",
    "Transformer architecture for reference:\n",
    "<div>\n",
    "<img src=\"https://d2l.ai/_images/transformer.svg\" width=\"275\"/>\n",
    "</div>\n",
    "\n",
    "First, there is the residual layer norm that appears after every other component. In all cases, this takes the output of the previous component, sums it with the input to that component (the residual connection), and then normalizes the result across the layer.\n",
    "\n",
    "Second is the positionwise feed forward network that appears after the attention components. It is a two layer fully connected module with a ReLU activation in between. \n",
    "\n",
    "These are provided for reference here. Note that dropout would normally be applied in various places in these modules during training, but we have omitted it for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the modules we need, we can begin assembling the bigger modules. First we will look at the Encoder Block. The actual encoder will be made up of some number of these stacked together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Encoder Block Module\n",
    "In this exercise you will create the forward method of the PyTorch module representing the Encoder Block of the Transformer. The Encoder Block has the following architecture:\n",
    "1. a multi-head attention module using self-attention\n",
    "2. 1st residual layer norm\n",
    "3. feed-forward model\n",
    "4. 2nd residual layer norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads, embed_dim)\n",
    "        self.norm1 = ResidualNorm(embed_dim)\n",
    "        self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
    "        self.norm2 = ResidualNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_tokens (n_batch, n_tokens, embed_dim): the source sequence\n",
    "          src_mask (n_batch, n_tokens): binary mask over the source\n",
    "        Returns:\n",
    "          (n_batch, n_tokens, embed_dim): the encoder state\n",
    "        \"\"\"\n",
    "        ####################################################################\n",
    "        # Fill in missing code below (...),\n",
    "        # then remove or comment the line below to test your function\n",
    "        raise NotImplementedError(\"EncoderBlock\")\n",
    "        ####################################################################\n",
    "        # First compute self-attention on the source tokens by passing them in\n",
    "        # as the queries, keys, and values to the attention module.\n",
    "        self_attention = ...\n",
    "        # Next compute the norm of the self-attention result with a residual\n",
    "        # connection from the source tokens\n",
    "        normed_attention = ...\n",
    "        # Pass the normed attention result through the feedforward component\n",
    "        ff_out = ...\n",
    "        # Finally compute the norm of the feedforward output with a residual\n",
    "        # connection from the normed attention output\n",
    "        out = ...\n",
    "        return out\n",
    "\n",
    "\n",
    "# Uncomment below to test your module\n",
    "# n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
    "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
    "# mask = torch.ones((batch_size, n_tokens))\n",
    "# encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
    "# encoder(tokens, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Encoder Block, we can chain these together in a stack to get the full Encoder module. We will include the embedding layer and positional encoding step of the source tokens here as well. The input to this module then will be a tensor of a batch of token IDs and corresponding mask.\n",
    "\n",
    "For instance, if our entire corpus was the English sentence: `Cat sat on the mat` and we tokenized by word, our vocab size would be 5 as there are 4 unique words. Converting this sentence to IDs would be `[[0,1,2,3,4]]`.\n",
    "\n",
    "The code for the Encoder module is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoder(embed_dim)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderBlock(n_heads, embed_dim, hidden_dim) for _ in range(n_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src_tokens, src_mask):\n",
    "        x = self.embedding(src_tokens)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, src_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Uncomment below to test your module\n",
    "# vocab_size = 5\n",
    "# n_blocks, n_heads, batch_size, embed_dim, hidden_dim = 10, 2, 1, 4, 8\n",
    "# enc = Encoder(vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
    "# src_tokens = torch.IntTensor([[0, 1, 2, 3, 4]])\n",
    "# src_mask = torch.IntTensor([[1, 1, 1, 1, 1]])\n",
    "# enc(src_tokens, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder\n",
    "\n",
    "Like the encoder, the decoder is made up of a stack of repeating Decoder Blocks. Decoder Blocks are similar to the Encoder ones with an additional multi-head attention component that doesn't use self-attention, but instead gets the queries from the decoder's self-attention component and the keys and values from the encoder's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Decoder Block Module\n",
    "In this exercise you will create the forward method of the PyTorch module representing the Decoder Block of the Transformer. The Decoder Block has the following architecture:\n",
    "1. a multi-head attention using self-attention\n",
    "2. 1st residual layer norm\n",
    "3. a 2nd multi-head attention that incorporates the encoder output\n",
    "4. 2nd residual layer norm\n",
    "5. feed-forward model\n",
    "6. 3rd residual layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(n_heads, embed_dim)\n",
    "        self.norm1 = ResidualNorm(embed_dim)\n",
    "        self.encoder_attention = MultiHeadAttention(n_heads, embed_dim)\n",
    "        self.norm2 = ResidualNorm(embed_dim)\n",
    "        self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
    "        self.norm3 = ResidualNorm(embed_dim)\n",
    "\n",
    "    def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_tokens (n_batch, n_tokens, embed_dim): the target sequence\n",
    "          tgt_mask (n_batch, n_tokens): binary mask over the target tokens\n",
    "          encoder_state (n_batch, n_tokens, embed_dim): the output of the encoder pass\n",
    "          src_mask (n_batch, n_tokens): binary mask over the source tokens\n",
    "        Returns:\n",
    "          (n_batch, n_tokens, embed_dim): the decoder state\n",
    "        \"\"\"\n",
    "        ####################################################################\n",
    "        # Fill in missing code below (...),\n",
    "        # then remove or comment the line below to test your function\n",
    "        raise NotImplementedError(\"DecoderBlock\")\n",
    "        ####################################################################\n",
    "        # First compute self-attention on the target tokens by passing them in\n",
    "        # as the queries, keys, and values to the attention module along with the\n",
    "        # target mask.\n",
    "        self_attention = ...\n",
    "        # Next compute the norm of the self-attention result with a residual\n",
    "        # connection from the target tokens\n",
    "        normed_self_attention = ...\n",
    "        # Compute the encoder attention by using the normed self-attention output as\n",
    "        # the queries and the encoder state as the keys and values along with the\n",
    "        # source mask.\n",
    "        encoder_attention = ...\n",
    "        # Next compute the norm of the encoder attention result with a residual\n",
    "        # connection from the normed self-attention\n",
    "        normed_encoder_attention = ...\n",
    "        # Pass the normed encoder attention result through the feedforward component\n",
    "        ff_out = ...\n",
    "        # Finally compute the norm of the feedforward output with a residual\n",
    "        # connection from the normed attention output\n",
    "        out = ...\n",
    "        return out\n",
    "\n",
    "\n",
    "# Uncomment below to test your module\n",
    "# n_heads, batch_size, n_tokens, embed_dim, hidden_dim = 2, 1, 3, 4, 8\n",
    "# tokens = torch.normal(0, 1, (batch_size, n_tokens, embed_dim))\n",
    "# src_mask = torch.ones((batch_size, n_tokens))\n",
    "# tgt_mask = torch.ones((batch_size, n_tokens))\n",
    "# encoder = EncoderBlock(n_heads, embed_dim, hidden_dim)\n",
    "# decoder = DecoderBlock(n_heads, embed_dim, hidden_dim)\n",
    "# encoder_state = encoder(tokens, src_mask)\n",
    "# decoder(tokens, tgt_mask, encoder_state, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder module ends up just the same as the Encoder module with one key difference: the forward method needs to also accept the output of the encoder as well as the source token mask.\n",
    "\n",
    "For instance, let's assume we are doing a translation task and want to translate the English `Cat sat on the mat` into the French `Chat assis sur le tapis`. Out target vocab size is also 5 and would be similarly converted into IDs as `[[0,1,2,3,4]]`.\n",
    "\n",
    "The code for the Decoder module is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoder(embed_dim)\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [DecoderBlock(n_heads, embed_dim, hidden_dim) for _ in range(n_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
    "        x = self.embedding(tgt_tokens)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, tgt_mask, encoder_state, src_mask)\n",
    "        return x\n",
    "\n",
    "# Uncomment below to test your module\n",
    "# vocab_size = 5\n",
    "# n_blocks, n_heads, batch_size, embed_dim, hidden_dim = 10, 2, 1, 4, 8\n",
    "# tgt_tokens = torch.IntTensor([[0, 1, 2, 3, 4]])\n",
    "# tgt_mask = src_mask = torch.IntTensor([[1, 1, 1, 1, 1]])\n",
    "# enc_state = torch.randn((1, 5, 4))\n",
    "# dec = Decoder(vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
    "# dec(tgt_tokens, tgt_mask, enc_state, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Transformer Model\n",
    "\n",
    "We can now put the Encoder and Decoder together to produce the full Transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Transformer Module\n",
    "\n",
    "In the last exercise for this section you will implement the forward method of the full Transformer module. First you will apply the source tokens and mask to the Encoder to get its output, then use that along with the target tokens and mask to produce the Decoder output. Finally we run the Decoder output through a linear layer to transform the embeddings back into vocab ID scores in order to determine the actual next word prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, src_vocab_size, tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
    "        self.out = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask, tgt_tokens, tgt_mask):\n",
    "        ####################################################################\n",
    "        # Fill in missing code below (...),\n",
    "        # then remove or comment the line below to test your function\n",
    "        raise NotImplementedError(\"Transformer\")\n",
    "        ####################################################################\n",
    "        # Compute the encoder output state from the source tokens and mask\n",
    "        encoder_state = ...\n",
    "        # Compute the decoder output state from the target tokens and mask as well\n",
    "        # as the encoder state and source mask\n",
    "        decoder_state = ...\n",
    "        # Compute the vocab scores by passing the decoder state through the output\n",
    "        # linear layer\n",
    "        out = ...\n",
    "        return out\n",
    "\n",
    "\n",
    "# Uncomment below to test your module\n",
    "# src_vocab_size = tgt_vocab_size = 5\n",
    "# n_blocks, n_heads, batch_size, embed_dim, hidden_dim = 10, 2, 1, 4, 8\n",
    "# src_tokens = tgt_tokens = torch.IntTensor([[0,1,2,3,4]])\n",
    "# src_mask = tgt_mask = torch.IntTensor([[1,1,1,1,1]])\n",
    "\n",
    "# transformer = Transformer(src_vocab_size, tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
    "# transformer(src_tokens, src_mask, tgt_tokens, tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We've covered all the building blocks that make up the Transformer network architecture from the attention mechanism up to the fully combined encoder-decoder setup. The module versions presented here were often simplified in some ways and made more verbose in others to emphasize what each component is doing.\n",
    "\n",
    "You should now feel that you have a good understanding of the Transformer architecture and how it works. In the next notebook we will look at how to train this model, and see what the current state of the art is for machine learning practitioners in industry, academia... and in the emerging world of large language models like GPT-3 and GPT-4.\n",
    "\n",
    "We hope that we have \"unmagicked\" the transformer architecture for you in the same way we unmagicked stable diffusion together. These world-changing models are just a combination of a few simple ideas that have been around for a long time!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Up\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**In a deep attention network, why would you expect part of speech tagging to be done closer to the input, and co-reference to be done more deeply in the network?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_pos_vs_coreference = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the original Transformer architecture, the authors used both Dropout _and_ output masking. Why do we need to mask some of the target tokens?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_and_masking = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your quiz answers (run this cell to submit)\n",
    "\n",
    "quizdt.store(\n",
    "    dict(\n",
    "        notebook=tutorial,\n",
    "        my_pennkey=my_pennkey,\n",
    "        my_pod=my_pod,\n",
    "        my_email=my_email,\n",
    "        why_k_embed_q_embed=why_k_embed_q_embed,\n",
    "        why_pos_vs_coreference=why_pos_vs_coreference,\n",
    "        dropout_and_masking=dropout_and_masking,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
