{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U numpy==1.22.4 datasets transformers evaluate datatops==0.2.2 vibecheck==0.0.3 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('d2pNpcbdTG0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SotA Models\n",
    "\n",
    "Like we've mentioned in previous notebooks, unless you are writing your own experimental DL research (and sometimes even then!) it is _far_ more common these days to use the HuggingFace model library to quickly import and start working with state of the art models. In this section we will show you how to do that.\n",
    "\n",
    "We will download a pretrained model from the hf `transformers` library that is used to generate text. We will then fine-tune it on a different dataset, using the `hf.datasets` library and the HuggingFace Trainer classes to make the process as easy as possible, and we'll see that we can accomplish all of this in just a few lines of easily maintained code.\n",
    "\n",
    "At the end, we will have a _working_ generator... for code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
    "my_pennkey = \"\"  # @param {type:\"string\"}\n",
    "my_pod = \"\"  # @param {type:\"string\"}\n",
    "my_email = \"\"  # @param {type:\"string\"}\n",
    "tutorial = \"W10D2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback setup (run this cell)\n",
    "\n",
    "# Feedback with Datatops\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from datatops import Datatops\n",
    "\n",
    "feedback_dtid = \"62a48t3w\"\n",
    "feedback_name = \"cis522_feedback\"\n",
    "quiz_dtid = \"lxx8szk1\"\n",
    "quiz_name = \"cis522_quiz\"\n",
    "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
    "\n",
    "# Instantiate the Datatops client\n",
    "dt = Datatops(dt_url)\n",
    "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're first going to pick a tokenizer. You can see some of the options [here](https://huggingface.co/transformers/pretrained_models.html). We'll use CodeParrot tokenizer, which is a BPE tokenizer. But you can choose (or build!) another if you'd like to try offroading! \n",
    "\n",
    "> **Quiz: Why can you use a different tokenizer than the one that was originally used? What requirements must another tokenizer for this task have?**\n",
    ">\n",
    "> Hint: You couldn't, for example, use the very popular `bert-base-uncased` tokenizer, even though it's a popular choice for text generation tasks that was trained on the English Wikipedia and the BookCorpus datasets (which are both available in the `hf.datasets` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_tokenizer_choice = \"\" #@param{type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll download a pre-built model architecture. CodeParrot (the model) is a GPT-2 model, which is a transformer-based language model. You can see some of the options [here](https://huggingface.co/transformers/pretrained_models.html). But you can choose (or build!) another!\n",
    "\n",
    "Note that `codeparrot/codeparrot` (https://huggingface.co/codeparrot/codeparrot) is about 7GB to download (so it may take a while, or it may be too large for your runtime if you're on a free Colab). Instead, we will use a smaller model, `codeparrot/codeparrot-small` (https://huggingface.co/codeparrot/codeparrot-small), which is only ~500MB.\n",
    "\n",
    "To run everything together — tokenization, model, and de-tokenization, we can use the `pipeline` function from `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\", # The task to run. This tells hf what the pipeline steps are\n",
    "    model=model, # The model to use; can also pass the string here;\n",
    "    tokenizer=tokenizer, # The tokenizer to use; can also pass the string name here.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = '''\\\n",
    "def simple_add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Adds two numbers together and returns the result.\n",
    "    \"\"\"'''\n",
    "\n",
    "# Return tensors for PyTorch:\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these tokens are integer indices into the vocabulary of the tokenizer. We can use the tokenizer to decode these tokens into a string, which we can then print out to see what the model is generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_ids = inputs[\"input_ids\"]\n",
    "input_strs = tokenizer.convert_ids_to_tokens(*input_token_ids.tolist())\n",
    "\n",
    "print(*zip(input_strs, input_token_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D2_Kickoff\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Quick knowledge-check: what are the weirdly-rendering characters representing?)**\n",
    "\n",
    "This model is already ready to use! Let's give it a try. (Note that we don't use `inputs` — we just generated that to show the initial tokenization steps.)\n",
    "\n",
    "Here, we use the `pipeline` that we created earlier to chain all of our components together. If you were writing a Copilot-style code-completer, you could get away with wrapping this single line in a nice API and calling it a day!\n",
    "\n",
    "Play with the hyperparameters and see what kinds of outputs you can get. Temperature is a measure of how much randomness is added to the model's predictions. Higher temperature means more randomness, and lower temperature means less randomness. More randomness in the latent space will lead to wilder predictions, but potentially more creative answers as well. A good place to start is `0.2`. You can also try changing the `max_length` parameter, which controls how long the generated code can be (though the model can opt to put a \"stop\" token in the middle of the sequence, so it may not always generate exactly this many tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generation_pipeline(input_prompt, max_length=..., num_return_sequences=1, temperature=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can fool our model now! The huggingface documentation tells us that the codeparrot model was trained to generate Python code ([docs](https://huggingface.co/codeparrot/codeparrot-small)). Let's see if we can get it to generate some JavaScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"class SimpleAdder {\"\n",
    "\n",
    "print(generation_pipeline(input_prompt, max_length=..., num_return_sequences=1, temperature=...)[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! I don't know what it generated for you, but what it made for me was:\n",
    "\n",
    "```\n",
    "class SimpleAdder {\n",
    "    public:\n",
    "        class SimpleAdder(object):\n",
    "            def __init__(self, a, b):\n",
    "                self.a = a\n",
    "                self.b = b\n",
    "\n",
    "            def __call__(self, x):\n",
    "                return self.a + x\n",
    "```\n",
    "\n",
    "**Ew!** That's wrong in a _lot_ of ways. But it's understandable: Our model can't really generalize outside of the domain in which it was trained. And so probably there were a few Python files that included syntax of other languages (perhaps generators for other code?) and so the model knows that there's some mysterious syntax that uses curly brackets... But it's not sure about anything else. (For the programming-language hobbyists among you: The `public` notation looks to me a lot like the model is trying to do something C-flavored and perhaps something Java-flavored; I like it! But it's definitely not JavaScript.)\n",
    "\n",
    "What are the major observations?\n",
    "\n",
    "* The syntax it's generating rapidly devolves into Python; it's able to predict only a few characters of non-Python before falling back on its familiar training territory.\n",
    "* The part of the code that follows Python syntax is valid, and even resembles a useful class definition. (Although if you look closely, it really doesn't seem to do anything useful with the `b` attribute...) This tells us that the model \"understands\" its problem domain, but just hasn't been trained on the right data to solve our new problem.\n",
    "\n",
    "**What are your other observations about the code it generated for you? You're now aware of how Transformers work. (1) Think specifically and remark about the observations a machine learning practitioner would make here if your role were to diagnose the error in a production system. Now, (2) how would a nonexpert user interpret the issues? (3) Do you think the model-reported confidence for this output would be high, low, in between...?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_domain_generation_observations = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D2_GPT2FailureModes\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Alright, so we have a model that can generate code. But now we want to fine-tune it to generate JavaScript.\n",
    "\n",
    "Assuming the data will be too large to fit on disk on Colab, we'll use the `load_dataset` function to download only part of the dataset.  There's actually a JavaScript subset to the codeparrot dataset, which we'll use as an example... but you can use any dataset you like! We recommend filtering datasets by task category (e.g. text generation) to get the most relevant datasets, but you can use any dataset you like if you can configure the data-loader to use it. (Consider, for example, [this one](https://huggingface.co/datasets/angie-chen55/javascript-github-code).)\n",
    "\n",
    "> **Choose a dataset from the HuggingFace datasets library:**\n",
    "> \n",
    "> https://huggingface.co/datasets?task_categories=task_categories:text-generation&sort=downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike _some_ code-generator models on the market, we'll limit our training data by license :)\n",
    "dataset = load_dataset(\n",
    "    \"codeparrot/github-code\",\n",
    "    streaming=True,\n",
    "    split=\"train\",\n",
    "    languages=[\"JavaScript\"],\n",
    "    licenses=[\"mit\", \"isc\", \"apache-2.0\"],\n",
    ")\n",
    "# Print the schema of the first example from the training set:\n",
    "print({k: type(v) for k, v in next(iter(dataset)).items()})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like training any model, we need to define a training loop and an evaluation metric. \n",
    "\n",
    "This is made overwhelmingly easy with the `transformers` library. Specifically, take a look below at all of the code you can avoid by using the huggingface infrastructure. (In the past, we've used PyTorch Lightning, which had a similar training-loop abstraction. Do you have preferences between these two libraries?)\n",
    "\n",
    "Here are the big pieces of what we do below:\n",
    "\n",
    "* **Create a `TrainingArguments` object.** This is a serializable object (i.e., you can save it to memory or to disk) that makes it easy to train a model reproducibly with the same hyperparameters. (This certainly beats having a bunch of global variables in your notebook!)\n",
    "* **Encode the dataset.** This is effectively just passing everything through the tokenizer, with a padding step that fills the end of each sequence with the padding token. \n",
    "* **Define our metrics.** We use the `accuracy` metric here (look at the 4th line in the code cell). Why might *accuracy* be a bad metric for this task? (Hint: What does it mean to be \"accurate\" in this task?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric_observations = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create a data collator.** This is a function that takes a list of examples and returns a batch of examples. The `DataCollatorForLanguageModeling` class is a convenient way to do this.\n",
    "* **Create a `Trainer` object.** This is a class that wraps the training loop and makes it easy to train a model. It's a bit like the `Trainer` class in PyTorch Lightning, but it's a bit more flexible, and works with non-PyTorch models as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "# Trainer:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codeparrot\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x[\"code\"], truncation=True, padding=\"max_length\"),\n",
    "    batched=True,\n",
    "    remove_columns=[\"code\"],\n",
    ")\n",
    "\n",
    "# Metrics for loss:\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Data collator:\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the actual training:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will try our model on the same code snippet to see how it performs after fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the CPU for inference:\n",
    "model.to(\"cpu\")\n",
    "print(\n",
    "    generation_pipeline(\n",
    "        input_prompt, max_length=100, num_return_sequences=1, temperature=0.2\n",
    "    )[0][\"generated_text\"]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, your results will be slightly different. Here's what I got:\n",
    "\n",
    "```javascript\n",
    "class SimpleAdder {\n",
    "    constructor(a, b) {\n",
    "        this.a = a;\n",
    "        this.b = b;\n",
    "    }\n",
    "\n",
    "    add(\n",
    "```\n",
    "\n",
    "Much better! The model is no longer generating Python code, and it's not trying to jam python-flavored syntax into other languages. It's still not perfect, but it's a lot better than before! (And of course, remember that this is just a small model, and we didn't train it for very long. You can try training it for longer, or using a larger model, to get better results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D2_Finetuning\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('m5pwM9z1jYg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_out_of_domain_example = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_and_diminishing_failures = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your quiz answers (run this cell to submit)\n",
    "\n",
    "quizdt.store(\n",
    "    dict(\n",
    "        notebook=tutorial,\n",
    "        my_pennkey=my_pennkey,\n",
    "        my_pod=my_pod,\n",
    "        my_email=my_email,\n",
    "        why_tokenizer_choice=why_tokenizer_choice,\n",
    "        out_of_domain_generation_observations=out_of_domain_generation_observations,\n",
    "        accuracy_metric_observations=accuracy_metric_observations,\n",
    "        best_out_of_domain_example=best_out_of_domain_example,\n",
    "        common_and_diminishing_failures=common_and_diminishing_failures,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('UouBsEU_e9M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W10D2_Discussion\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
