{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DIazMknk1W6P"
      },
      "source": [
        "# CIS-522 Week 11 Part 1\n",
        "# Introduction to Reinforcement Learning\n",
        "\n",
        "__Instructor:__ Dinesh Jayaraman\n",
        "\n",
        "__Content creators:__ Chuning Zhu\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install numpy matplotlib tqdm vibecheck datatops IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5NQBmTeSs5S"
      },
      "outputs": [],
      "source": [
        "#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
        "my_pennkey = '' #@param {type:\"string\"}\n",
        "my_pod = '' #@param {type:\"string\"}\n",
        "my_email = '' #@param {type:\"string\"}\n",
        "tutorial = 'W11D1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfcCTgt0QMnm"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHBnw4Nah8JZ"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import IPython\n",
        "from numbers import Number\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm.auto import tqdm\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "from datatops import Datatops\n",
        "\n",
        "feedback_dtid = \"62a48t3w\"\n",
        "feedback_name = \"cis522_feedback\"\n",
        "quiz_dtid = \"lxx8szk1\"\n",
        "quiz_name = \"cis522_quiz\"\n",
        "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
        "\n",
        "# Instantiate the Datatops client\n",
        "dt = Datatops(dt_url)\n",
        "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iNi1AkDhh5Lm"
      },
      "outputs": [],
      "source": [
        "# @title Plotting functions\n",
        "%matplotlib inline \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "# Plotting functions\n",
        "def plot_episode_rewards(episode_rewards):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xFnuAj4MQ7sg"
      },
      "outputs": [],
      "source": [
        "# @title Gridworld Environment\n",
        "\n",
        "\"\"\"\n",
        "    A custom Gridworld environment with deterministic transitions. Adapted from \n",
        "    CS 188 Gridworld env. There are four actions: up, left, down, right. The \n",
        "    state is the (x, y) coordinates in the Grid. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Gridworld:\n",
        "    def __init__(self, grid, living_reward=-1.0):\n",
        "        self.h = len(grid)\n",
        "        self.w = len(grid[0])\n",
        "        self.living_reward = living_reward\n",
        "\n",
        "        self.scale = math.ceil(max(self.h, self.w) / min(self.h, self.w))\n",
        "        self.action_space = [\"up\", \"left\", \"down\", \"right\"]\n",
        "        self.n_actions = 4\n",
        "\n",
        "        self.init_grid(grid)\n",
        "\n",
        "    def init_grid(self, grid):\n",
        "        # Create reward grid. The reward grid is a numpy array storing the\n",
        "        # reward given for entering each state.\n",
        "        self.rew_grid = np.array(\n",
        "            [\n",
        "                [self.living_reward if isinstance(e, str) else e for e in row]\n",
        "                for row in grid\n",
        "            ],\n",
        "            dtype=np.float,\n",
        "        )\n",
        "\n",
        "        # Create grid. The grid is a numpy of chars.\n",
        "        # S (start), T (terminal), C (cliff), # (block), or ' ' (regular).\n",
        "        convert_fn = lambda e: \"T\" if e >= self.living_reward else \"C\"\n",
        "        self.grid = np.array(\n",
        "            [\n",
        "                [convert_fn(e) if isinstance(e, Number) else e for e in row]\n",
        "                for row in grid\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Find initial state\n",
        "        start_indices = np.argwhere(self.grid == \"S\")\n",
        "        if len(start_indices) == 0:\n",
        "            raise Exception(\"Grid has no start state\")\n",
        "        self.init_state = (start_indices[0][1], start_indices[0][0])\n",
        "\n",
        "    def get_transition(self, state, action):\n",
        "        \"\"\"\n",
        "        Execute one action in the environment.\n",
        "        Args:\n",
        "            state (tuple): the (x, y) coordinates of the current state.\n",
        "            action (int): the current action chosen from {0, 1, 2, 3}.\n",
        "\n",
        "        Returns:\n",
        "            next_state (tuple): the (x, y) coordinates of the next state.\n",
        "            reward (float): the reward for the current time step.\n",
        "        \"\"\"\n",
        "        # Handle terminal states\n",
        "        x, y = state\n",
        "        if self.grid[y, x] == \"T\":\n",
        "            return state, 0\n",
        "\n",
        "        # Handle invalid actions\n",
        "        if action not in range(len(self.action_space)):\n",
        "            raise Exception(\"Illegal action\")\n",
        "\n",
        "        # Default transitions\n",
        "        named_action = self.action_space[action]\n",
        "        nx, ny = x, y\n",
        "        if named_action == \"up\":\n",
        "            ny -= 1\n",
        "        elif named_action == \"left\":\n",
        "            nx -= 1\n",
        "        elif named_action == \"down\":\n",
        "            ny += 1\n",
        "        elif named_action == \"right\":\n",
        "            nx += 1\n",
        "\n",
        "        # Handle special cases\n",
        "        if nx < 0 or nx >= self.w or ny < 0 or ny >= self.h or self.grid[ny, nx] == \"#\":\n",
        "            # Give living reward if next state is blocked or out of bounds\n",
        "            reward = self.living_reward\n",
        "            next_state = (x, y)\n",
        "        else:\n",
        "            reward = self.rew_grid[ny, nx]\n",
        "            if self.grid[ny, nx] == \"C\":\n",
        "                next_state = self.init_state  # falls off cliff\n",
        "            else:\n",
        "                next_state = (nx, ny)  # transition to next state\n",
        "\n",
        "        return next_state, reward\n",
        "\n",
        "    def __render(self):\n",
        "        # Render grid with matplotlib patches.\n",
        "        fig, ax = plt.subplots(figsize=(self.h * self.scale, self.w * self.scale))\n",
        "        ax.set_aspect(\"equal\")\n",
        "        ax.set_xlim(0, self.w)\n",
        "        ax.set_ylim(0, self.h)\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.tick_params(length=0)\n",
        "        plt.axis(\"off\")\n",
        "        for y in range(self.h):\n",
        "            for x in range(self.w):\n",
        "                cell_type = self.grid[y, x]\n",
        "                if cell_type == \"S\":\n",
        "                    c = \"#DAE8FC\"  # blue\n",
        "                elif cell_type == \"#\":\n",
        "                    c = \"#CCCCCC\"  # gray\n",
        "                elif cell_type == \"T\":\n",
        "                    c = \"#D5E8D4\"  # green\n",
        "                elif cell_type == \"C\":\n",
        "                    c = \"#F8CECC\"  # red\n",
        "                else:\n",
        "                    c = \"#FFFFFF\"  # white\n",
        "                rect = patches.Rectangle(\n",
        "                    (x, self.h - y - 1), 1, 1, fc=c, ec=\"gray\", lw=1\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "        return fig, ax\n",
        "\n",
        "    def render_grid(self):\n",
        "        fig, ax = self.__render()\n",
        "        for y in range(self.h):\n",
        "            for x in range(self.w):\n",
        "                if self.grid[y, x] != \"#\":\n",
        "                    # alternate: x+0.1, self.h-y-0.2\n",
        "                    ax.text(\n",
        "                        x + 0.5,\n",
        "                        self.h - y - 0.5,\n",
        "                        str(self.rew_grid[y, x]),\n",
        "                        size=\"medium\",\n",
        "                        ha=\"center\",\n",
        "                        va=\"center\",\n",
        "                    )\n",
        "        plt.title(\"Rewards\")\n",
        "        fig.show()\n",
        "\n",
        "    def render_values(self, V):\n",
        "        fig, ax = self.__render()\n",
        "        for y in range(self.h):\n",
        "            for x in range(self.w):\n",
        "                ax.text(\n",
        "                    x + 0.5,\n",
        "                    self.h - y - 0.5,\n",
        "                    \"{:.2f}\".format(V[y, x]),\n",
        "                    size=\"medium\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                )\n",
        "        plt.title(\"Values\")\n",
        "        fig.show()\n",
        "\n",
        "    def render_q_values(self, Q):\n",
        "        fig, ax = self.__render()\n",
        "        for y in range(self.h):\n",
        "            for x in range(self.w):\n",
        "                named_action = self.action_space[np.argmax(Q[y, x])]\n",
        "                xl, xc, xr = x, x + 0.5, x + 1\n",
        "                yt, yc, yb = self.h - y, self.h - y - 0.5, self.h - y - 1\n",
        "                ce, tl, bl, tr, br = [xc, yc], [xl, yt], [xl, yb], [xr, yt], [xr, yb]\n",
        "                if named_action == \"up\":\n",
        "                    xy = np.array([ce, tl, tr])\n",
        "                elif named_action == \"left\":\n",
        "                    xy = np.array([ce, tl, bl])\n",
        "                elif named_action == \"down\":\n",
        "                    xy = np.array([ce, bl, br])\n",
        "                elif named_action == \"right\":\n",
        "                    xy = np.array([ce, br, tr])\n",
        "                ax.plot([x, x + 1], [self.h - y, self.h - y - 1], \"gray\", lw=1)\n",
        "                ax.plot([x, x + 1], [self.h - y - 1, self.h - y], \"gray\", lw=1)\n",
        "                poly = patches.Polygon(xy, True, fc=\"#FFFF00\", ec=\"gray\")\n",
        "                ax.add_patch(poly)\n",
        "                ax.text(\n",
        "                    x + 0.5,\n",
        "                    self.h - y - 0.2,\n",
        "                    \"{:.2f}\".format(Q[y, x, 0]),\n",
        "                    size=\"small\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                )\n",
        "                ax.text(\n",
        "                    x + 0.2,\n",
        "                    self.h - y - 0.5,\n",
        "                    \"{:.2f}\".format(Q[y, x, 1]),\n",
        "                    size=\"small\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                )\n",
        "                ax.text(\n",
        "                    x + 0.5,\n",
        "                    self.h - y - 0.8,\n",
        "                    \"{:.2f}\".format(Q[y, x, 2]),\n",
        "                    size=\"small\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                )\n",
        "                ax.text(\n",
        "                    x + 0.8,\n",
        "                    self.h - y - 0.5,\n",
        "                    \"{:.2f}\".format(Q[y, x, 3]),\n",
        "                    size=\"small\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                )\n",
        "        fig.show()\n",
        "        plt.title(\"Q-values\")\n",
        "        pass\n",
        "\n",
        "    def render_policy(self, policy):\n",
        "        fig, ax = self.__render()\n",
        "        for y in range(self.h):\n",
        "            for x in range(self.w):\n",
        "                if policy[y, x] not in range(len(self.action_space)):\n",
        "                    raise Exception(\"Illegal action\")\n",
        "                if self.grid[y, x] == \"T\":\n",
        "                    continue\n",
        "                arrow_len = 0.3\n",
        "                dx, dy = 0, 0\n",
        "                named_action = self.action_space[policy[y, x]]\n",
        "                if named_action == \"up\":\n",
        "                    dy = arrow_len\n",
        "                elif named_action == \"left\":\n",
        "                    dx = -arrow_len\n",
        "                elif named_action == \"down\":\n",
        "                    dy = -arrow_len\n",
        "                elif named_action == \"right\":\n",
        "                    dx = arrow_len\n",
        "                arrow = patches.FancyArrow(\n",
        "                    x + 0.5, self.h - y - 0.5, dx, dy, 0.03, True, color=\"#6C8EBF\"\n",
        "                )\n",
        "                ax.add_patch(arrow)\n",
        "        plt.title(\"Policy\")\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    GridworldEnv is a wrapper around Gridworld implementing an RL interface.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class GridworldEnv(Gridworld):\n",
        "    def __init__(self, grid, living_reward=-1.0):\n",
        "        super().__init__(grid, living_reward)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the agent to its initial state\n",
        "        \"\"\"\n",
        "        self.state = self.init_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one action in the environment.\n",
        "        Args:\n",
        "            action (int): the current action chosen from {0, 1, 2, 3}.\n",
        "\n",
        "        Returns:\n",
        "            next_state (tuple): (x, y) coordinates of the next state.\n",
        "            reward (float): reward for the current time step.\n",
        "            done (bool): True if a terminal state has been reached, False otherwise.\n",
        "        \"\"\"\n",
        "        next_state, reward = self.get_transition(self.state, action)\n",
        "        self.state = next_state\n",
        "        done = self.grid[self.state[1], self.state[0]] == \"T\"\n",
        "        return next_state, reward, done\n",
        "\n",
        "\n",
        "# Pre-defined grids\n",
        "def get_book_grid():\n",
        "    grid = [\n",
        "        [\"T\", \" \", \" \", \" \"],\n",
        "        [\" \", \" \", \" \", \" \"],\n",
        "        [\" \", \" \", \" \", \" \"],\n",
        "        [\"S\", \" \", \" \", \"T\"],\n",
        "    ]\n",
        "    return GridworldEnv(grid)\n",
        "\n",
        "\n",
        "def get_cliff_small():\n",
        "    grid = [\n",
        "        [\" \", \" \", \" \", \" \", \" \"],\n",
        "        [\"S\", \" \", \" \", \" \", \"T\"],\n",
        "        [-100, -100, -100, -100, -100],\n",
        "    ]\n",
        "    return GridworldEnv(grid)\n",
        "\n",
        "\n",
        "def get_cliff_walk():\n",
        "    grid = [[\" \" for _ in range(12)] for _ in range(3)]\n",
        "    grid.append([-100 for _ in range(12)])\n",
        "    grid[3][0] = \"S\"\n",
        "    grid[3][-1] = \"T\"\n",
        "    return GridworldEnv(grid)\n",
        "\n",
        "\n",
        "def get_bridge_grid():\n",
        "    grid = [\n",
        "        [\"#\", -100, -100, -100, -100, -100, \"#\"],\n",
        "        [1, \"S\", \" \", \" \", \" \", \" \", 10],\n",
        "        [\"#\", -100, -100, -100, -100, -100, \"#\"],\n",
        "    ]\n",
        "    return GridworldEnv(grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrwdQ2uZ326v"
      },
      "source": [
        "---\n",
        "# Section 1: Introduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "BSJWWkvq0xUv",
        "outputId": "fbd0043c-527c-4029-8d86-7e550e8d909d"
      },
      "outputs": [],
      "source": [
        "#@title Video : Intro to Reinforcement Learning\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"cVTud58UfpQ\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tOvIv7bd0ssI"
      },
      "source": [
        "Up to this point, we have mainly been concerned with supervised learning. In a supervised learning problem, we are provided with a dataset where each sample comes with a ground truth label (e.g. class label), and the goal is to learn to predict the label by minimizing some loss function. Reinforcement learning, on the other hand, is a framework for solving sequential decision-making problems. Consider an agent operating in some environment. The agent's goal is to carry out the best sequence of actions that maximizes the cumulative reward. This is difficult because the action at the current time step influences future states of the environment, which then feed back to the agent's observations. The following figure illustrates this setting. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/693511/228991389-37d600c6-8e7d-4a3c-bfe0-ef2b5704c08a.png\" alt=\"RL setting\" />\n",
        "\n",
        "What is the role of reinforcement learning in intelligence? According to Yann LeCun, if intelligence is a cake, then unsupervised learning is the bulk of the cake, supervised learning the icing, and reinforcement learning the cherry on top. One of the arguments why RL takes up such a small proportion is that very little learning in real world comes with explicit extrinsic reward signal. This analogy is still debatable, as some RL folks argue that intelligence is more like a cake with lots of cherries on top. The idea being that there need not be a single explicit reward but rather there are methods that obtain reward signals from (self-)set goals; one example would be [hindsight experience replay](https://arxiv.org/abs/1707.01495). Consider for instance how a student studies for an exam. Often they will set themselves a goal to understand the first chapter of a book before they go to the second chapter and then, they progressively continue until they have studied all the material to pass the exam. In addition, there are ways to solve sequential decision making problems without relying on meticulously designed rewards, such as inverse reinforcement learning which, for instance, extracts a reward function from experience.\n",
        "\n",
        "Another way to put RL in perspective is by comparing it with vision and natural language processing. If we decompose intelligence into perception, cognition (reasoning), and action (decision making), then vision coarsely corresponds to perception, NLP cognition, and RL action. Just like how vision can be combined with NLP for tasks like image captioning, RL can be organically combined with vision and NLP as well. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/693511/228991391-44b78dd5-ee07-4a6f-81a7-014a02234daa.jpeg\" alt=\"Cherry cake\" />\n",
        "\n",
        "In this first tutorial, we will briefly step away from deep learning and study a few classic approaches in reinforcement learning. A good reference is Sutton and Barto's book, Reinforcement Learning: An Introduction. The [full text](http://incompleteideas.net/book/the-book.html) is avaliable online."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89WTR6ApXHvk"
      },
      "source": [
        "---\n",
        "# Section 2: MDP and Bellman Equations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMGCAwaxfOta"
      },
      "source": [
        "## Section 2.1: Markov Decision Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "qx4b3ejy1OD9",
        "outputId": "cdd40a94-bf3b-4216-c227-26399450827e"
      },
      "outputs": [],
      "source": [
        "#@title Video : Markov Decision Processes\n",
        "\n",
        "video = YouTubeVideo(id=\"GJEL-QkT2yk\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw9tTf82fA-4"
      },
      "source": [
        "\n",
        "We begin our study of reinforcement learning with a definition of  Markov decision process. A Markov decision process (MDP) is a tuple $(S, A, P, R, \\gamma)$, where\n",
        "\n",
        "- $S$ is the set of **states**.\n",
        "- $A$ is the set of **actions**.\n",
        "- $P$ defines the **transition probablities**. $P(s'|s, a)$ gives the probability of transitioning to state $s'$ by taking action $a$ at state $s$. \n",
        "- $R$ is the **reward function**. $R(s, a)$ gives the reward of taking action $a$ at state $s$. $R$ can also be a function of state only.\n",
        "- $\\gamma$ is the **discount factor**. It controls how much future rewards matter to us. We will talk more about discount factor in the next video.\n",
        "\n",
        "As an aside, we introduce partially observable MDP (POMDP). A POMDP additionally has a set of obervations $O$ and emission probabilities $\\varepsilon$. $\\varepsilon(o|s)$ gives the probability of observing $o$ at state $s$. This formulation is useful when we don't have access to explicit state information, but are provided with observations that may not fully reveal the underlying states. An example is reinforcement learning from images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi-ZCWHcfSGS"
      },
      "source": [
        "## Section 2.2 Solving MDPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "vqUOUj4B29aT",
        "outputId": "0a412e1d-214f-4206-d3f9-0f9b8d1570e8"
      },
      "outputs": [],
      "source": [
        "#@title Video : Solving MDPs\n",
        "\n",
        "video = YouTubeVideo(id=\"meywaLPitZ4\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O16_8EVc21Jz"
      },
      "source": [
        "\n",
        "A policy $\\pi: S â†¦ A$ is a function representing a distribution mapping states to actions. At state $s$, we sample an action $a$ from the distribution $\\pi(a|s)$ to execute in the environment. If all probability mass is assigned to one action, then the policy is deterministic. The goal of reinforcement learning is to find an optimal policy $\\pi^*$ that maximizes the expected sum of discounted rewards: $$J_{\\pi} = E_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^tR(s_t, a_t)\\right]$$\n",
        "\n",
        "Note that this objective assumes a continuous task, i.e. that $t$ extends to infinity. We can generalize it to episodic tasks with finite horizons by replacing $\\infty$ with task horizon $T$. We may also discard the discount factor $\\gamma$ in an episodic task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XxYIO5H4WAk"
      },
      "source": [
        "## Section 2.3: Bellman Equations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "x1eA0Uc22tRT",
        "outputId": "f54fdb03-c249-446d-fef4-c0704de8831f"
      },
      "outputs": [],
      "source": [
        "#@title Video: V, Q, and the Bellman Equation\n",
        "\n",
        "video = YouTubeVideo(id=\"tm39P5jT320\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLGIBaui4IfK"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "We define the value of a state $s$ under policy $\\pi$ as the expected future reward for following $\\pi$ starting from $s$, $$V^{\\pi}(s) = E_{\\pi} \\left[\\sum_{t'=t}^{\\infty} \\gamma^{t'-t}R(s_{t'}, a_{t'}) \\mid s_t = s\\right] .$$\n",
        "\n",
        "We further define the value of a state-action pair $(s, a)$ under policy $\\pi$ as the expected future reward for taking action $a$ at state $s$, *and then* following $\\pi$. This is also known as the Q-value $$Q^{\\pi}(s, a) = E_{\\pi} \\left[\\sum_{t'=t}^{\\infty} \\gamma^{t'-t}R(s_{t'}, a_{t'}) \\mid s_t = s, a_t = a\\right].$$\n",
        "\n",
        "Observe that $V$ and $Q$ can be related by a simple equation:\n",
        "$$V^{\\pi}(s) = E_{a\\sim \\pi(a|s)}\\left[Q^{\\pi}(s, a)\\right]$$\n",
        "\n",
        "By definition, $V$ and $Q$ satisfy the following Bellman equations.\n",
        "\n",
        "\\begin{align*}\n",
        "V^{\\pi}(s) &= E_{a \\sim \\pi(a|s)}\\left[R(s, a)+ \\gamma E_{s' \\sim P(s'|s, a)} \\left[V^{\\pi}(s')\\right]\\right] \\\\\n",
        "Q^{\\pi}(s, a) &= R(s, a) + \\gamma E_{s' \\sim P(s'|s, a)}\\left[ E_{a' \\sim \\pi(a'|s')}\\left[Q(s', a')\\right]\\right]\n",
        "\\end{align*}\n",
        "\n",
        "The optimal value function $V^*$ captures the expected future reward if we start from state $s$ and act optimally in the future. Similarly, the optimal Q-function $Q^*$ captures the expected future reward if we start from state $s$, take action $a$, and then act optimally in the future. They satisfy the Bellman optimality equations:   \n",
        "\n",
        "\\begin{align*}\n",
        "V^*(s) &= \\max_{a\\in A}\\left(R(s, a) + \\gamma E_{s' \\sim P(s'|s, a)} \\left[V^*(s')\\right]\\right)\\\\\n",
        "Q^*(s, a) &= R(s, a) + \\gamma E_{s' \\sim P(s'|s, a)} \\left[ \\max_{a' \\in A} Q^*(s', a')\\right]\n",
        "\\end{align*}\n",
        "\n",
        "If we have learned the optimal value function $V^*$ or Q-function $Q^*$, we can infer an optimal (deterministic) policy known as the greedy policy or argmax policy: $$\\pi(s) = \\arg\\max_{a\\in A}Q^*(s, a)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title .\n",
        "DatatopsContentReviewContainer(\n",
        "    \"\",\n",
        "    \"W11D1_Bellman\",\n",
        "    {\n",
        "        \"url\": dt_url,\n",
        "        \"name\": feedback_name,\n",
        "        \"user_key\": feedback_dtid,\n",
        "    }\n",
        ").render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpQmkho4CMK"
      },
      "source": [
        "---\n",
        "# Section 3: Gridworld\n",
        "\n",
        "\n",
        "Before we dive into RL algorithms, let's get familiar with the running example we will use throughout this tutorial -- the Gridworld environment. As its name suggests, the Gridworld environment is an $m \\times n$ grid. The states are x-y coordinates in the grid, with origin at the top-left corner. The agent starts from the initial state and aims to reach the terminal state. There are four actions: up, left, down, and right. Each action leads to a **deterministic** transition to the adjacent cell in the correspond direction (i.e. $P(s'|s, a) = 1$). By default, a reward of -1 is issued for entering any non-terminal state, although our implementation allows you to define an arbitrary reward for each state. \n",
        "\n",
        "To be more specific, our implementation admits four special cells: 'S' (start), 'T' (terminal), 'C' (cliff), and '#' (block). They are colored blue, green, red, and gray respectively. The agent spawns at the start cell, and aims to reach the terminal cell. If the agent \"falls off the cliff,\" it will get a high penalty (-100) and be sent back to the start cell. If the agent tries to enter a block cell or go out of the grid, it will instead stay at the same place and get a reward of -1. \n",
        "\n",
        "Familiarize yourself with the environment's interface by interacting with the following code cells. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qVmIOoR2BtIV",
        "outputId": "886f69c2-c59a-4d9f-a8ed-991d1cfa7484"
      },
      "outputs": [],
      "source": [
        "# Get a pre-defined grid\n",
        "gw = get_cliff_small()\n",
        "\n",
        "# Render rewards \n",
        "gw.render_grid()\n",
        "\n",
        "# Render random values\n",
        "values = np.random.rand(4, 12)\n",
        "gw.render_values(values)\n",
        "\n",
        "# Render random Q-values and argmax policy\n",
        "q_values = np.random.randn(4, 12, 4)\n",
        "gw.render_q_values(q_values)\n",
        "\n",
        "# Render random policy\n",
        "policy = np.random.choice(4, (4, 12)).astype(int)\n",
        "gw.render_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcY20RwdmF68"
      },
      "source": [
        "In our Gridworld environment, states are represented by a tuple (x, y), and actions are encoded as 0, 1, 2, 3, corresponding to up, left, down, right. `reset()` resets the agent to its initial state and returns the initial state. `step(action)` executes an action in the environment. It returns the agent's next state, the reward, and a boolean value indicating whether or not the terminal state is reached. In the following cell, control the agent to reach the terminal state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su9q2dZcDXIa",
        "outputId": "0a6caf28-c92a-43f6-ded2-194ca944938e"
      },
      "outputs": [],
      "source": [
        "action_space = ['up', 'left', 'down', 'right']\n",
        "def gw_step(gw, a):\n",
        "    next_state, reward, done = gw.step(a)\n",
        "    print(f'You moved {action_space[a]} to {next_state}, reward: {reward}, terminal state reached: {done}')\n",
        "\n",
        "print(f\"Initial state: {gw.reset()}\") # reset to initial state\n",
        "gw_step(gw, 0) # move up\n",
        "gw_step(gw, 2) # move down\n",
        "\n",
        "# Use gw_step() to reach the terminal state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o60dzYCbmUQ0"
      },
      "source": [
        "A useful method of the `Gridworld` class is `get_transition(state, action)`. It takes in a state and an action and returns the next state and the reward. We will use this function for exercises 1-3 where we assume full knowledge of the environment's transitions. In a reinforcement learning setting, we only have access to `step(action)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q013KaKmTx_",
        "outputId": "156e13be-765a-4872-ca52-e64e0703995f"
      },
      "outputs": [],
      "source": [
        "# Show next state and reward for each action at state (0, 1)\n",
        "print(gw.get_transition((0, 1), 0))\n",
        "print(gw.get_transition((0, 1), 1))\n",
        "print(gw.get_transition((0, 1), 2))\n",
        "print(gw.get_transition((0, 1), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IbGOBZ0XW4Q"
      },
      "source": [
        "---\n",
        "# Section 4 Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "5XUdhM8f4UyS",
        "outputId": "a4dfd282-cee9-4bba-ae25-79a7c4fcb242"
      },
      "outputs": [],
      "source": [
        "#@title Video : Policy and Value Iteration\n",
        "video = YouTubeVideo(id=\"l87rgLg90HI\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic_SJemA4C85"
      },
      "source": [
        "## Section 4.1: Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cThkPW0pN81H"
      },
      "source": [
        "If we have full knowledge of the environment, in particular its transitions, we can use dynamic programming to find the optimal policy. The first algorithm we will study is policy iteration. We start with policy evaluation, which computes the value function of the policy using the Bellman equation. We iteratively perform Bellman backup for the value of each state until convergence: \n",
        "$$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\left(R(s, a) + \\gamma\\sum_{s'}P(s'|s, a)V(s')\\right) $$\n",
        "Since we have deterministic transitions, this simplifies to \n",
        "$$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\left(R(s, a) + \\gamma V(s')\\right)$$\n",
        "where $s'$ is the state we transition to by taking action $a$ at state $s$. \n",
        "\n",
        "In the following excercise, you will evaluate a random policy which assigns equal probablities to all actions at each state. Complete one step of a Bellman backup. You can get the next state and reward using `grid.get_transition((x, y), action)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ska_I22ua1jd"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GOVgQpKRhzL"
      },
      "outputs": [],
      "source": [
        "# Random Policy evaluation\n",
        "def random_policy_evaluation(grid, gamma=1.0):\n",
        "    values = np.zeros_like(grid.rew_grid)\n",
        "    iter = 0\n",
        "    while True:\n",
        "        eps = 0\n",
        "        for y in range(grid.h):\n",
        "            for x in range(grid.w):\n",
        "                v = values[y, x]\n",
        "                new_v = 0\n",
        "                for action in range(grid.n_actions):\n",
        "                    ###########################################################\n",
        "                    # Fill in missing code below (...),\n",
        "                    # then remove or comment the line below to test your function\n",
        "                    raise NotImplementedError(\"Random policy evaluation\")\n",
        "                    ###########################################################\n",
        "                    (new_x, new_y), reward = grid.get_transition((x, y), action)\n",
        "                    new_v += ...\n",
        "\n",
        "                values[y, x] = new_v\n",
        "                eps = max(eps, abs(new_v - v))\n",
        "        iter += 1\n",
        "        if eps < 0.0001:\n",
        "            print(\"Converged after {} iterations\".format(iter))\n",
        "            break\n",
        "    return values\n",
        "\n",
        "\n",
        "# # Uncomment to test\n",
        "# grid = get_book_grid()\n",
        "# values = random_policy_evaluation(grid)\n",
        "# grid.render_values(values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBpGwHYwE5-Z"
      },
      "source": [
        "Now we move on to the policy iteration algorithm. Policy iteration consists of two steps: policy evaluation and policy improvement. We first evaluate the policy, and then use the new values to derive a better policy by selecting the greedy action at each state. These steps are repeated until convergence. For an analysis of the theoretical guarantees of policy iteration, see [this page](http://incompleteideas.net/book/first/ebook/node42.html).\n",
        "\n",
        "In the following exercise, you will implement the policy iteration algorithm. For policy evaluation, note that we have a deterministic greedy policy, so there's no need to iterate over actions. The general backup rule becomes $$V(s) \\leftarrow R(s, \\pi(s)) + \\gamma\\sum_{s'} P(s'|s, \\pi(s)) V(s')$$ and for our environment this is $V(s) \\leftarrow R(s, \\pi(s)) + \\gamma V(s')$. For policy improvement, we do the same evaluation for all actions and store them in the action_values array, from which we derive the greedy policy. **Be careful when indexing into the value matrix**: `values[y, x]` stores the value of state (x, y)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj97dbrea5cz"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7PNDZT-bsql"
      },
      "outputs": [],
      "source": [
        "# Policy Iteration\n",
        "def policy_evaluation(grid, values, policy, gamma):\n",
        "    while True:\n",
        "        eps = 0\n",
        "        for y in range(grid.h):\n",
        "            for x in range(grid.w):\n",
        "                v = values[y, x]\n",
        "                (new_x, new_y), reward = grid.get_transition((x, y), policy[y, x])\n",
        "                ################################################################\n",
        "                # Fill in missing code below (...),\n",
        "                # then remove or comment the line below to test your function\n",
        "                raise NotImplementedError(\"Policy evaluation\")\n",
        "                ################################################################\n",
        "                new_v = ...\n",
        "\n",
        "                values[y, x] = new_v\n",
        "                eps = max(eps, abs(new_v - v))\n",
        "        if eps < 0.0001:\n",
        "            break\n",
        "\n",
        "\n",
        "def policy_improvement(grid, values, policy, gamma):\n",
        "    converged = True\n",
        "    for y in range(grid.h):\n",
        "        for x in range(grid.w):\n",
        "            old_action = policy[y, x]\n",
        "            action_values = np.zeros(grid.n_actions, dtype=np.float)\n",
        "            ####################################################################\n",
        "            # Fill in missing code below (...),\n",
        "            # then remove or comment the line below to test your function\n",
        "            raise NotImplementedError(\"Policy improvement\")\n",
        "            ####################################################################\n",
        "            for action in range(...):\n",
        "                (new_x, new_y), reward = grid.get_transition((x, y), action)\n",
        "                action_values[action] = ...\n",
        "            policy[y, x] = ...\n",
        "\n",
        "            if old_action != policy[y, x]:\n",
        "                converged = False\n",
        "    return converged\n",
        "\n",
        "\n",
        "def policy_iteration(grid, gamma=0.9):\n",
        "    policy = np.random.choice(grid.n_actions, (grid.h, grid.w)).astype(int)\n",
        "    values = np.zeros_like(grid.rew_grid)\n",
        "    converged = False\n",
        "    while not converged:\n",
        "        print(\"running policy evaluation\")\n",
        "        policy_evaluation(grid, values, policy, gamma)\n",
        "        print(\"running policy improvement\")\n",
        "        converged = policy_improvement(grid, values, policy, gamma)\n",
        "    return values, policy\n",
        "\n",
        "\n",
        "# # Uncomment to test\n",
        "# grid = get_book_grid()\n",
        "# values, policy = policy_iteration(grid)\n",
        "# grid.render_values(values)\n",
        "# grid.render_policy(policy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title .\n",
        "DatatopsContentReviewContainer(\n",
        "    \"\",\n",
        "    \"W11D1_DynamicProgramming\",\n",
        "    {\n",
        "        \"url\": dt_url,\n",
        "        \"name\": feedback_name,\n",
        "        \"user_key\": feedback_dtid,\n",
        "    }\n",
        ").render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2trCXzobQfV"
      },
      "source": [
        "## Quiz Question 1\n",
        "\n",
        "Construct the path from the policy visualization and see that following the policy from the initial state indeed leads to terminal state. Now change $\\gamma$ to 1.0 and rerun the code. Does policy iteration still converge? Why are we stuck on policy evaluation? (This is a brain-teaser, so don't spend too much time on it, and don't let the code run for too long.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eAiaKleibZcA"
      },
      "outputs": [],
      "source": [
        "convergence = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwFYqD3kjE6h"
      },
      "source": [
        "## Section 4.2: Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDDeaHezfMve"
      },
      "source": [
        "Value iteration can be thought of as a simplification of policy iteration, where we effectively combine the two steps in policy iteration into one. We still iterate over all states, but in each iteration the value update becomes $$V(s) \\leftarrow \\max_a R(s, a) + \\gamma\\sum_{s'}P(s'|s, a)V(s') $$ So instead of computing the state value and then selecting the greedy action, we directly store the maximum state-action value. This obviates the need to maintain an explicit policy. After the value matrix has converged, we can back out the optimal policy by taking the argmax, same as what we did in policy improvement.\n",
        "\n",
        "Now it's your turn to implement the value iteration algorithm. You need to fill in the new update rule, and copy your code from policy improvment to reconstruct the optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKCgDH5a89u"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckLBXw2AtqfR"
      },
      "outputs": [],
      "source": [
        "# Value Iteration\n",
        "\n",
        "\n",
        "def value_iteration(grid, gamma=0.9):\n",
        "    V = np.zeros_like(grid.rew_grid)\n",
        "    while True:\n",
        "        eps = 0\n",
        "        for y in range(grid.h):\n",
        "            for x in range(grid.w):\n",
        "                v = values[y, x]\n",
        "                action_values = np.zeros(grid.n_actions)\n",
        "                ################################################################\n",
        "                # Fill in missing code below (...),\n",
        "                # then remove or comment the line below to test your function\n",
        "                raise NotImplementedError(\"Value iteration\")\n",
        "                ################################################################\n",
        "                for action in range(...):\n",
        "                    (nx, ny), reward = grid.get_transition((x, y), action)\n",
        "                    action_values[action] = ...\n",
        "                new_v = ...\n",
        "\n",
        "                values[y, x] = new_v\n",
        "                eps = max(eps, abs(new_v - v))\n",
        "        if eps < 0.0001:\n",
        "            break\n",
        "\n",
        "    # Create greedy policy from values\n",
        "    policy = np.zeros_like(grid.rew_grid).astype(int)\n",
        "    for y in range(grid.h):\n",
        "        for x in range(grid.w):\n",
        "            action_values = np.zeros(grid.n_actions)\n",
        "            ####################################################################\n",
        "            # Copy your solution for policy improvement here\n",
        "            raise NotImplementedError(\"Value iteration policy\")\n",
        "            ####################################################################\n",
        "            for action in range(...):\n",
        "                (nx, ny), reward = grid.get_transition((x, y), action)\n",
        "                action_values[action] = ...\n",
        "            policy[y, x] = ...\n",
        "\n",
        "    return values, policy\n",
        "\n",
        "\n",
        "# # Uncomment to test\n",
        "# grid = get_book_grid()\n",
        "# values, policy = value_iteration(grid)\n",
        "# grid.render_values(values)\n",
        "# grid.render_policy(policy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqEGcKG7MNBN"
      },
      "source": [
        "---\n",
        "# Section 5: Temporal Difference (TD) Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "WWz3Nvcs4pSa",
        "outputId": "7d4e2d53-9669-4bac-e721-8a58ff2ecb1a"
      },
      "outputs": [],
      "source": [
        "# @title Video : TD and Q Learning\n",
        "video = YouTubeVideo(id=\"rCk_hvwZ6iA\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0cMYLme3-3U"
      },
      "source": [
        "## Section 5.1 Q-learning\n",
        "\n",
        "Up until now we have assumed full access to the transitions of an environment. But in a typical reinforcement learning problem the dynamics is unknown. So how do we solve it? One way is to learn to approximate the dynamics using a function approximator (e.g. a neural net) and then apply dynamic programming or trajectory optimization. This is called model-based reinforcement learning, which we will cover next week. In this tutorial, we will study algorithms in the model-free regime. Specifically, we will investigate **Temporal Difference (TD) learning**.\n",
        "\n",
        "The idea behind TD learning is to use $V(s_{t+1})$ as an imperfect proxy for the true value (Monte Carlo bootstrapping), and obtain a generalized equation to calculate the TD error:\n",
        "$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
        "\n",
        "The expression $r_{t+1} + \\gamma V(s_{t+1})$ is also called the TD target. We can then update the value using a learning rate $\\alpha$.\n",
        "$$ V(s_t) \\leftarrow V(s_t) + \\alpha \\delta_t$$\n",
        "\n",
        "**Q-learning** is an instantiation of TD learning, where the TD error is $$\\delta_t = R(s_t, a_t) + \\gamma \\max_{a} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$$ and the full update rule is $$Q(s_t,a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(R(s_t, a_t) + \\gamma \\max_{a} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right)$$\n",
        "\n",
        "Because of the max operator used to select the optimal Q-value in the TD target, Q-learning directly estimates the optimal action value, i.e. the cumulative future reward that would be obtained if the agent behaved optimally, regardless of the policy currently followed by the agent. For this reason, Q-learning is referred to as an **off-policy** method. A sketch of the Q-learning algorithm is as follows:\n",
        "\n",
        "```\n",
        "for n episodes:\n",
        "    for T steps:\n",
        "        Select an action a_t using some policy derived from the current Q-values\n",
        "        Execute a_t in the environment to get reward r and next state s_{t+1}\n",
        "        Update Q(s_t, a_t) using (s_t, a_t, r, s_{t+1})\n",
        "```\n",
        "\n",
        "A remaining question is, how do we select an action base on the current Q-values? If the approximated Q-values are very bad, then greedily following the argmax policy may cause the agent to get stuck in some bad states. Thus, we instead adopt an **epsilon-greedy policy**, where we choose the argmax action with probability $(1-\\epsilon)$ and take a random action otherwise. This relates to an important concept in reinforcement learning, namely exploration vs. exploitation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EehisZexviwX"
      },
      "outputs": [],
      "source": [
        "# Epsilon-greedy policy\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "    if np.random.random() > epsilon:\n",
        "        action = np.argmax(q_values)\n",
        "    else:\n",
        "        action = np.random.choice(len(q_values))\n",
        "    return action\n",
        "\n",
        "\n",
        "# General TD learning algorithm\n",
        "def learn_gridworld(env, backup_rule, params, max_steps, n_episodes):\n",
        "    values = np.zeros((env.h, env.w, env.n_actions))\n",
        "    episode_actions = []\n",
        "    episode_rewards = np.zeros(n_episodes)\n",
        "\n",
        "    for episode in tqdm(range(n_episodes)):\n",
        "        env.reset()\n",
        "        total_reward = 0\n",
        "        action_list = []\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            state = env.state\n",
        "            # Select action from epsilon-greedy policy\n",
        "            action = epsilon_greedy(values[state[1], state[0]], params[\"epsilon\"])\n",
        "            action_list.append(action)\n",
        "            # Execute action\n",
        "            next_state, reward, done = env.step(action)\n",
        "            # Update values\n",
        "            values = backup_rule(state, action, reward, next_state, values, params)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        episode_actions.append(action_list)\n",
        "        episode_rewards[episode] = total_reward\n",
        "\n",
        "    return values, episode_rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpnKs95GbFdC"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6QbhpiSiLub"
      },
      "source": [
        "In this exercise, you will implement the update rule for Q-learning and test it on the Cliff World environment, where the agent needs to navigate to the other side of the cliff without falling off. You need to fill in the code for computing the TD error and updating the values matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTZGFohOOvYD"
      },
      "outputs": [],
      "source": [
        "# Q-Learning\n",
        "def q_learning_backup(state, action, reward, next_state, values, params):\n",
        "    \"\"\"\n",
        "    Compute a new set of q-values using the q-learning update rule.\n",
        "    Args:\n",
        "        state (tuple): s_t, a tuple of xy coordinates.\n",
        "        action (int): a_t, an integer from {0, 1, 2, 3}.\n",
        "        reward (float): the reward of executing a_t at s_t.\n",
        "        next_state (tuple): s_{t+1}, a tuple of xy coordinates.\n",
        "        values (ndarray): an (h, w, 4) numpy array of q-values. values[y, x, a]\n",
        "                          stores the value of executing action a at state (x, y).\n",
        "        params (dict): a dictionary of parameters.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: the updated q-values.\n",
        "    \"\"\"\n",
        "    x, y = state\n",
        "    nx, ny = next_state\n",
        "    gamma = params[\"gamma\"]\n",
        "    alpha = params[\"alpha\"]\n",
        "\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Q-learning\")\n",
        "    ####################################################################\n",
        "    q = ...\n",
        "    max_next_q = ...\n",
        "\n",
        "    # Compute TD error using q and max_next_q\n",
        "    td_error = ...\n",
        "    values[y, x, action] = ...\n",
        "\n",
        "    return values\n",
        "\n",
        "\n",
        "# # Uncomment to test\n",
        "# env = get_cliff_walk()\n",
        "# params = {'gamma': 1.0, 'alpha': 0.1 , 'epsilon': 0.1}\n",
        "# max_steps = 1000\n",
        "# n_episodes = 500\n",
        "# q_values, episode_rewards = learn_gridworld(env, q_learning_backup, params, max_steps, n_episodes)\n",
        "\n",
        "# plot_episode_rewards(episode_rewards)\n",
        "# env.render_policy(np.argmax(q_values, axis=2))\n",
        "# env.render_q_values(q_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_3P3ZRB3896"
      },
      "source": [
        "## Section 5.2: SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM7fOpXcUbIa"
      },
      "source": [
        "An alternative to Q-learning, the SARSA algorithm also estimates action values. However, rather than estimating the optimal (off-policy) values, SARSA estimates the **on-policy** action values, i.e. the cumulative future reward that would be obtained if the agent behaved according to its current beliefs.\n",
        "\n",
        "\\begin{align}\n",
        "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(R(s_t, a_t) + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)\\big)\n",
        "\\end{align}\n",
        "\n",
        "In fact, you will notices that the *only* difference between Q-learning and SARSA is the TD target calculation uses the policy to select the next action (in our case epsilon-greedy) rather than using the action that maximizes the Q-value. You do not need to implement the SARSA algorithm. Run the following code cell and compare with Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "af809a19556b464b8fc78bceef261e18",
            "0dd4200e25164f3993da34f464a0fcf8",
            "bf6c780380054accb8c23ad560876133",
            "b58cffc817874f14b20eab6d387aaea7",
            "42b0dbba73d74c94b4cb9866089251bc",
            "6ef785a055d547fbb04627c15fa55c94",
            "9cd3270179dc4bc6a83334063ee74897",
            "0fb89dec9b1c4c82bbf505e921998035",
            "36203069fe7e4defa24d46affa988cc8",
            "45687b571af94c0e9fd65a0fc4ae7e59",
            "0f7c9bc3046c4990a625a52cd72b4a86"
          ]
        },
        "id": "lP3OrNyRt-K3",
        "outputId": "172dcdd8-00a2-49a4-bb66-23ea5b369c14"
      },
      "outputs": [],
      "source": [
        "# SARSA\n",
        "def sarsa_backup(state, action, reward, next_state, values, params):\n",
        "    \"\"\"\n",
        "    Compute a new set of q-values using the SARSA update rule.\n",
        "    Args:\n",
        "        state (tuple): s_t, a tuple of xy coordinates.\n",
        "        action (int): a_t, an integer from {0, 1, 2, 3}.\n",
        "        reward (float): the reward of executing a_t at s_t.\n",
        "        next_state (tuple): s_{t+1}, a tuple of xy coordinates.\n",
        "        values (ndarray): an (h, w, 4) numpy array of q-values. values[y, x, a]\n",
        "            stores the value of executing action a at state (x, y).\n",
        "        params (dict): a dictionary of parameters.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: the updated q-values.\n",
        "    \"\"\"\n",
        "    x, y = state\n",
        "    nx, ny = next_state\n",
        "    gamma = params[\"gamma\"]\n",
        "    alpha = params[\"alpha\"]\n",
        "\n",
        "    q = values[y, x, action]\n",
        "    # Obtain on-policy action\n",
        "    policy_action = epsilon_greedy(values[ny, nx], params[\"epsilon\"])\n",
        "    next_q = values[ny, nx, policy_action]\n",
        "\n",
        "    # Compute TD error using q and max_next_q\n",
        "    td_error = reward + (gamma * next_q - q)\n",
        "    values[y, x, action] = q + alpha * td_error\n",
        "\n",
        "    return values\n",
        "\n",
        "\n",
        "env = get_cliff_walk()\n",
        "params = {\"gamma\": 1.0, \"alpha\": 0.1, \"epsilon\": 0.1}\n",
        "max_steps = 1000\n",
        "n_episodes = 500\n",
        "q_values, episode_rewards = learn_gridworld(\n",
        "    env, sarsa_backup, params, max_steps, n_episodes\n",
        ")\n",
        "\n",
        "plot_episode_rewards(episode_rewards)\n",
        "env.render_policy(np.argmax(q_values, axis=2))\n",
        "env.render_q_values(q_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title .\n",
        "DatatopsContentReviewContainer(\n",
        "    \"\",\n",
        "    \"W11D1_TD_Q_SARSA\",\n",
        "    {\n",
        "        \"url\": dt_url,\n",
        "        \"name\": feedback_name,\n",
        "        \"user_key\": feedback_dtid,\n",
        "    }\n",
        ").render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpkSH_BK49-b"
      },
      "source": [
        "---\n",
        "# Wrap-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "SkggStBmyOnQ",
        "outputId": "bdf3eb67-2a22-4985-818c-97374b66afc9"
      },
      "outputs": [],
      "source": [
        "# @title Submit your quiz answers (run this cell to submit)\n",
        "\n",
        "quizdt.store(\n",
        "    {\n",
        "        \"my_pennkey\": my_pennkey,\n",
        "        \"my_pod\": my_pod,\n",
        "        \"my_email\": my_email,\n",
        "        \"tutorial\": tutorial,\n",
        "        \"convergence\": convergence,\n",
        "    }\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CIS_522_W12D1_Tutorial_â€“_Student_Version.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dd4200e25164f3993da34f464a0fcf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef785a055d547fbb04627c15fa55c94",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9cd3270179dc4bc6a83334063ee74897",
            "value": "100%"
          }
        },
        "0f7c9bc3046c4990a625a52cd72b4a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fb89dec9b1c4c82bbf505e921998035": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36203069fe7e4defa24d46affa988cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42b0dbba73d74c94b4cb9866089251bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45687b571af94c0e9fd65a0fc4ae7e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef785a055d547fbb04627c15fa55c94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cd3270179dc4bc6a83334063ee74897": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af809a19556b464b8fc78bceef261e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dd4200e25164f3993da34f464a0fcf8",
              "IPY_MODEL_bf6c780380054accb8c23ad560876133",
              "IPY_MODEL_b58cffc817874f14b20eab6d387aaea7"
            ],
            "layout": "IPY_MODEL_42b0dbba73d74c94b4cb9866089251bc"
          }
        },
        "b58cffc817874f14b20eab6d387aaea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45687b571af94c0e9fd65a0fc4ae7e59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0f7c9bc3046c4990a625a52cd72b4a86",
            "value": " 500/500 [00:01&lt;00:00, 587.50it/s]"
          }
        },
        "bf6c780380054accb8c23ad560876133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb89dec9b1c4c82bbf505e921998035",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36203069fe7e4defa24d46affa988cc8",
            "value": 500
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
