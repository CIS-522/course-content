{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U datatops==0.2.2 gensim==4.3.1 matplotlib==3.7.1 matplotlib-inline==0.1.3 numpy==1.24.2 requests==2.28.2 requests-oauthlib==1.3.0 scikit-learn==1.2.2 vibecheck==0.0.3 > /dev/null 2>&1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings: Preface\n",
    "\n",
    "* **Authors**: Jordan Matelsky\n",
    "* **Reviewers**: Lyle Ungar, Konrad Kording\n",
    "\n",
    "Today we are going to study word embeddings, a technique that allows us to represent text as numerical inputs or outputs to a neural network. **Note that you don't need to have a GPU for today's tutorial.**\n",
    "\n",
    "> * This notebook will use a new feedback mechanism to get vibe-checks throughout the notebook; please do feel encouraged to click the feedback buttons to submit anonymous feedback! This will not count toward your grade.\n",
    "> * In general, Jupyter notebooks should follow good code-style practices, and all the imports should go at the top. But because much of this code might be useful to you in the future, we will leave cells with imports in them throughout the notebook so that it is easy to copy and paste to reuse this code. For more information on notebook best practices, see [here](https://gist.github.com/j6k4m8/864fb5a8a2257237169a48b6bd6a307e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback setup (run this cell)\n",
    "\n",
    "# Feedback with Datatops\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from datatops import Datatops\n",
    "\n",
    "feedback_dtid = \"62a48t3w\"\n",
    "feedback_name = \"cis522_feedback\"\n",
    "quiz_dtid = \"lxx8szk1\"\n",
    "quiz_name = \"cis522_quiz\"\n",
    "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
    "\n",
    "# Instantiate the Datatops client\n",
    "dt = Datatops(dt_url)\n",
    "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pennkey = \"\" #@param {type:\"string\"}\n",
    "my_pod = \"\" #@param {type:\"string\"}\n",
    "my_email = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the corpus. For this notebook, we're going to use the copy of Moby Dick that's available on Project Gutenberg.\n",
    "import requests\n",
    "\n",
    "RAW_CORPUS = \"\".join([\n",
    "    char.lower() for char in requests.get(\"https://www.gutenberg.org/files/2701/2701-0.txt\").text\n",
    "    if char.isalpha() or char in \" \\n\\t\"\n",
    "]).split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings\n",
    "\n",
    "In the course so far, we've discussed how to represent images as vectors. This has usually been pretty easy; we've just considered each pixel as its own dimension, and imagined an image as simply a folded-up long vector. We've also discussed low-dimension representations of larger vectors, like when we used PCA or VAEs to reduce the dimensionality of our images to a smaller latent space.\n",
    "\n",
    "Today we'll discuss how to do the same thing for text. We're going to implement a few different ways to embed text, and we'll see the advantages and disadvantages of each. \n",
    "\n",
    "Each embedding will follow this [protocol](https://en.wikipedia.org/wiki/Protocol_(object-oriented_programming)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Protocol\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WordEmbedder(Protocol):\n",
    "\n",
    "    def fit(self, sentence_corpus: List[List[str]], **kwargs) -> None:\n",
    "        ...\n",
    "\n",
    "    def transform(self, sentence: List[str]) -> np.ndarray:\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first think about the most naive thing we could do; a one-hot encoding!\n",
    "\n",
    "A one-hot encoding is a vector of length $V$, where $V$ is the size of the vocabulary. The vector is all zeros, except for a single 1, which is in the position of the word's index in the vocabulary. Let's build a simple example:\n",
    "\n",
    "```python\n",
    ">>> VOCAB = ['the', 'cat', 'ate', 'fish', 'dog']\n",
    "```\n",
    "\n",
    "Now, let's encode the sentence \"the cat ate the fish\":\n",
    "\n",
    "```\n",
    "vocab: the cat ate fish dog\n",
    "THE    1   0   0   0    0   \n",
    "CAT    0   1   0   0    0\n",
    "ATE    0   0   1   0    0\n",
    "THE    1   0   0   0    0\n",
    "FISH   0   0   0   1    0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder(WordEmbedder):\n",
    "\n",
    "    _vocab: List[str]\n",
    "\n",
    "    def fit(self, sentence_corpus: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Create a vocabulary from the corpus.\n",
    "\n",
    "        Arguments:\n",
    "            sentence_corpus: A list of sentences, where each sentence is a list of words.\n",
    "\n",
    "        \"\"\"\n",
    "        self._vocab = list(set(word for sentence in sentence_corpus for word in sentence))\n",
    "        \n",
    "\n",
    "    def transform(self, sentence: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode a sentence as a one-hot vector.\n",
    "\n",
    "        Arguments:\n",
    "            sentence: A list of words.\n",
    "\n",
    "        Returns:\n",
    "            A one-hot vector for the sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            [int(word.lower() == vocab_word) for vocab_word in self._vocab]\n",
    "            for word in sentence\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_embedder = OneHotEncoder()\n",
    "ohe_embedder.fit([RAW_CORPUS])\n",
    "for vec in ohe_embedder.transform(\"This fellow wants to beat up a whale\".split()):\n",
    "    unit_location = np.where(vec == 1)[0][0]\n",
    "    print(f\"[{unit_location} zeros... 1 ...a lot more zeros]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very easy way to represent text, but it has a few problems. First, it's very sparse. We have a lot of zeros in our vectors, and we're wasting a lot of space. Second, it's not very informative. We don't really know anything about the relationships between words. For example, there's no way for us to indicate that \"cat\" and \"dog\" are similar words, or that \"fish\" (noun) and \"ate\" (verb) are dissimilar. In a \"good\" embedding, we'd like to be able to capture these relationships and say that two words have a similar meaning if they're close together in the embedding space.\n",
    "\n",
    "Right now, all of our words are equally far apart, because each word is orthogonal to every other word!\n",
    "\n",
    "## Quiz!\n",
    "\n",
    "**We could fix the sparsity problem by using PCA to drop the vectors into a smaller space. Why wouldn't that be a useful embedding for our purposes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_ohe_pca_isnt_useful = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Using the word's surroundings in the corpus\n",
    "\n",
    "Here's another approach: We could look at a word's surroundings, and clump words with similar surroundings into the same part of the embedding space. For example, if we had a dataset that included the sentences,\n",
    "\n",
    "```\n",
    "...\n",
    "The cat ate the snack\n",
    "The dog ate the snack\n",
    "...\n",
    "```\n",
    "\n",
    "...then it would be reasonable to imagine that \"cat\" and \"dog\" have something in common (they're snack-eaters!), and so they should embed — at least along some dimensions — close to each other. \n",
    "\n",
    "How can we formalize this fill-in-the-blanks embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ContextBasedEmbedder:\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        sentences: List[List[str]],\n",
    "        window_size: int = 2,\n",
    "        learning_rate: float = 0.001,\n",
    "        epochs: int = 10,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Train the model using SGD.\n",
    "\n",
    "        Arguments:\n",
    "            sentences: A list of sentences, where each sentence is a list of words.\n",
    "            window_size: The number of words to the left and right of the center word to consider.\n",
    "            learning_rate: The learning rate for the gradient descent.\n",
    "            epochs: The number of epochs to train for.\n",
    "\n",
    "        \"\"\"\n",
    "        # Create a vocabulary, which is a sorted list of unique words:\n",
    "        self.vocab = list(\n",
    "            sorted(set(word for sentence in sentences for word in sentence))\n",
    "        )\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        # Make it easy to convert between words and indices:\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "\n",
    "        # Create the context windows:\n",
    "        windows = []\n",
    "        for sentence in sentences:\n",
    "            for center_word_index, center_word in enumerate(sentence):\n",
    "                for offset in range(-window_size, window_size + 1):\n",
    "                    context_word_index = center_word_index + offset\n",
    "                    if (\n",
    "                        context_word_index < 0\n",
    "                        or context_word_index >= len(sentence)\n",
    "                        or offset == 0\n",
    "                    ):\n",
    "                        continue\n",
    "                    context_word = sentence[context_word_index]\n",
    "                    windows.append(\n",
    "                        (\n",
    "                            self.word_to_index[center_word],\n",
    "                            self.word_to_index[context_word],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        self.embeddings = np.random.rand(vocab_size, self.embedding_dim)\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for center_word, context_word in windows:\n",
    "                # Embed the center and context words:\n",
    "                center_embed = self.embeddings[center_word]\n",
    "                context_embed = self.embeddings[context_word]\n",
    "\n",
    "                # Compute the dot product and the sigmoid output. This is the\n",
    "                # probability that the context word is the correct output:\n",
    "                dot_product = np.dot(center_embed, context_embed)\n",
    "                sigmoid_output = 1 / (1 + np.exp(-dot_product))\n",
    "\n",
    "                # Compute the gradients, which are the difference between the\n",
    "                # sigmoid output and 1:\n",
    "                gradient_center_embed = (sigmoid_output - 1) * context_embed\n",
    "                gradient_context_embed = (sigmoid_output - 1) * center_embed\n",
    "\n",
    "                # Update the embeddings using the gradients and the learning \n",
    "                # rate that we set earlier:\n",
    "                self.embeddings[center_word] -= learning_rate * gradient_center_embed\n",
    "                self.embeddings[context_word] -= learning_rate * gradient_context_embed\n",
    "\n",
    "                loss += -np.log(sigmoid_output)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\", end=\"\\r\")\n",
    "\n",
    "    def transform(self, word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the embedding for a word.\n",
    "\n",
    "        Arguments:\n",
    "            word: The word to get the embedding for (a string)\n",
    "\n",
    "        Returns:\n",
    "            The embedding for the word as a numpy array of shape (embedding_dim,).\n",
    "\n",
    "        \"\"\"\n",
    "        word = word.lower().strip()\n",
    "        ...\n",
    "        raise NotImplementedError(\"Implement this method!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the embedder.\n",
    "# TODO: Play with these hyperparameters to see how they affect the embeddings;\n",
    "#       you can also try changing the corpus at the top of the notebook.\n",
    "sg_embedder = ContextBasedEmbedder(embedding_dim=...)\n",
    "sg_embedder.fit([RAW_CORPUS], epochs=5, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\n",
    "    \"This book is a story about a man who wants to beat up a whale so he goes out in a boat and it takes a long time\"\n",
    ").split()\n",
    "embedding = [sg_embedder.transform(w) for w in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_2d = PCA(n_components=2).fit_transform(embedding)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(\n",
    "    embedding_2d[:, 0],\n",
    "    embedding_2d[:, 1],\n",
    "    c=\"red\"\n",
    ")\n",
    "for i, word in enumerate(sentence):\n",
    "    plt.annotate(word, (embedding_2d[i, 0], embedding_2d[i, 1]))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest words in the sentence:\n",
    "\n",
    "def closest_words(embedder, word, n=5):\n",
    "    word_embedding = embedder.transform(word)\n",
    "    distances = np.linalg.norm(embedder.embeddings - word_embedding, axis=1)\n",
    "    closest_indices = np.argsort(distances)[:n]\n",
    "    return [embedder.index_to_word[i] for i in closest_indices]\n",
    "\n",
    "\n",
    "closest_words(sg_embedder, \"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W9D1_Custom_Word2Vec\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright — let's be real; no one ever trains their own word embeddings like this on some tiny dataset. Like most reasonable people in machine learning, we're going to use a pre-trained embedding. (Remember in the stable diffusion notebook — even the authors of latent diffusion used off-the-shelf embeddings!)\n",
    "\n",
    "One of the most common pre-trained embeddings is called [GloVe](https://nlp.stanford.edu/projects/glove/). Let's explore some pre-trained embeddings below.\n",
    "\n",
    "### Further Reading for this Section\n",
    "\n",
    "- [Word2Vec Tutorial](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "- [Word2Vec vs GloVe](https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GloVe embeddings from gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_model = api.load('glove-twitter-25')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One fascinating byproduct of word embeddings is that we can do all sorts of interesting \"word arithmetic.\" For example, now \"distances\" between words is well-defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out some other words and see what the embeddings look like!\n",
    "\n",
    "words = {\n",
    "    \"fish\": glove_model[\"fish\"],\n",
    "    \"mermaid\": glove_model[\"mermaid\"],\n",
    "    \"shark\": glove_model[\"shark\"],\n",
    "    \"boy\": glove_model[\"boy\"],\n",
    "    \"girl\": glove_model[\"girl\"],\n",
    "    \"prince\": glove_model[\"prince\"],\n",
    "    \"princess\": glove_model[\"princess\"],\n",
    "    \"knight\": glove_model[\"knight\"],\n",
    "}\n",
    "\n",
    "glove_vecs_2d = PCA(n_components=2).fit_transform(\n",
    "    list(words.values())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distance between fish and boy:\", np.linalg.norm(words[\"fish\"] - words[\"boy\"]))\n",
    "print(\"Distance between fish and shark:\", np.linalg.norm(words[\"fish\"] - words[\"shark\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, more conceptually similar words are closer together in the embedding space. Even more interesting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4), dpi=150)\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(\n",
    "        glove_vecs_2d[i, 0],\n",
    "        glove_vecs_2d[i, 1],\n",
    "        c=\"red\"\n",
    "    )\n",
    "    plt.annotate(word, (glove_vecs_2d[i, 0], glove_vecs_2d[i, 1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(WARNING: we are of course looking at some very low dimensional shadows of the true embedding space! Caution is advised!)\n",
    "\n",
    "To go from the \"prince\" to \"princess\" vector, we go up and to the right. To go from \"boy\" to \"girl,\" we do a similar transformation. And to go from \"knight\" to \"mermaid...\" Alright, well maybe a \"fantasy-novel traditionally-male-flavored chracter\" and a \"fantasy-novel traditionally-female-flavored chracter\"...? \n",
    "\n",
    "Let's try some other math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(\"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the word for \"ethanol\" added to the concept of fun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the closest word to this new vector:\n",
    "glove_model.similar_by_vector((glove_model[\"ethanol\"] + glove_model[\"fun\"]), topn=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crabs are to the sea as `_____` are to the desert?\n",
    "\n",
    "(Analogy format: crab:sea :: ?:desert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies with GloVe embeddings:\n",
    "crab = glove_model[\"crab\"]\n",
    "seas = glove_model[\"seas\"]\n",
    "desert = glove_model[\"desert\"]\n",
    "\n",
    "# crab:seas :: desert:?\n",
    "glove_model.similar_by_vector(crab - seas + desert, topn=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...uh.\n",
    "\n",
    "Not quite.\n",
    "\n",
    "Turns out, the famous \"King - Man + Woman = Queen\" analogy — which was commonly used to show how word vectorization has intuitive superpowers — doesn't really work either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies with GloVe embeddings:\n",
    "king = glove_model[\"king\"]\n",
    "man = glove_model[\"man\"]\n",
    "woman = glove_model[\"woman\"]\n",
    "\n",
    "# king:man :: woman:?\n",
    "glove_model.similar_by_vector(king - man + woman, topn=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more reading, check out [this great blog post](https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85). And If you really want to dive into the science of word embeddings some more, you can explore the differences between the algorithm we wrote today — the CBOW model, where we use $n$ future and past words to predict the current word — and the [Skip-gram model](https://paperswithcode.com/method/skip-gram-word2vec), where we use the current word to predict its context words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But nowhere are the failings of these context-free embeddings more plain than when you find homonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"purple\", \"pink\", \"brown\", \"black\", \"gray\", \"yellow\"]\n",
    "color_vecs = [glove_model[col] for col in colors]\n",
    "fruits = [\n",
    "    \"apple\",\n",
    "    \"banana\",\n",
    "    \"pear\",\n",
    "    \"grape\",\n",
    "    \"strawberry\",\n",
    "    \"blueberry\",\n",
    "    \"raspberry\",\n",
    "    \"cherry\",\n",
    "]\n",
    "fruit_vecs = [glove_model[frt] for frt in fruits]\n",
    "\n",
    "orange = glove_model[\"orange\"]\n",
    "\n",
    "glove_pca = PCA(n_components=2)\n",
    "glove_pca.fit(np.concatenate([color_vecs, fruit_vecs, [orange]]))\n",
    "\n",
    "colors_2d = glove_pca.transform([glove_model[color] for color in colors])\n",
    "fruits_2d = glove_pca.transform([glove_model[fruit] for fruit in fruits])\n",
    "orange_2d = glove_pca.transform([orange])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4), dpi=150)\n",
    "plt.scatter(colors_2d[:, 0], colors_2d[:, 1], c=\"red\", s=5)\n",
    "plt.scatter(fruits_2d[:, 0], fruits_2d[:, 1], c=\"green\", s=5)\n",
    "plt.scatter(orange_2d[:, 0], orange_2d[:, 1], c=\"orange\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor orange :( It doesn't know if it's a color or a fruit!\n",
    "\n",
    "And that makes sense; in our current model, a word can only have one embedding. So if we have a word with multiple meanings, like \"orange\", we wind up _smearing_ its meanings into one vector.\n",
    "\n",
    "What we really want is a context-dependent embedding. That is, not only do we want to create the embedding for a word based upon its context; we also want to embed each _instance_ of a word in a target dataset based upon the words we find around it. In our ideal embedding, if we see the sentence \"I love to drink orange juice\", the vector for `orange` should be different than the vector for `orange` in the sentence \"I don't believe in orange rhinoceroses\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W9D1_Homonyms\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought Experiments!\n",
    "\n",
    "**Try asking your favorite large language model (e.g., https://chat.openai.com or Bing Chat) a question that includes a made-up word. For example, \"What is the most snooooglylooking puppy?\" Is the model able to recover the meaning of the word? Clearly the model has to embed your question and its words somehow... In a sentence or two, how would you build such an embedding algorithm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fabricated_embeddings = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So far, all of our embedding techniques have embedded single words at a time, even though we've trained on full sentences or even full stories. In a sentence or two, how would you write a sentence-embedding algorithm? What properties would you want this embedding system to have? (For example, in the word-embedding case, we wanted similar-meaning words to be close; or we wanted words that could fill the same sentence blank to be close together...)** \n",
    "\n",
    "Feel free to play around with the code in this notebook; some example code is provided in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectorization = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder:\n",
    "    def __init__(self, word_embedder: WordEmbedder):\n",
    "        self._embedder = word_embedder\n",
    "\n",
    "    def transform(self, sentence):\n",
    "        words = [word.lower() for word in sentence.split()]\n",
    "        word_embeddings = [self._embedder.transform(word)[0] for word in words]\n",
    "        # TODO: combine the word embeddings into a single sentence embedding\n",
    "        raise NotImplementedError(\"Implement the `combined_embedding` line below.\")\n",
    "        combined_embedding = ...\n",
    "        return combined_embedding\n",
    "\n",
    "\n",
    "emb = OneHotEncoder()  # or ContextBasedEmbedder()\n",
    "emb.fit([RAW_CORPUS])\n",
    "semb = SentenceEmbedder(emb)\n",
    "sentences = [\n",
    "    \"where should we have dinner\",\n",
    "    \"what is your favorite food\",\n",
    "    \"what is your favorite color\",\n",
    "    \"i am eating dinner over here\",\n",
    "    \"a red apple is a fruit\",\n",
    "    \"a green apple is a fruit\",\n",
    "    \"a red apple is not a dinner\",\n",
    "]\n",
    "sentence_embeddings_2d = PCA(2).fit_transform(\n",
    "    [semb.transform(sentence) for sentence in sentences]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(4, 4), dpi=150)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.scatter(sentence_embeddings_2d[i, 0], sentence_embeddings_2d[i, 1], c=\"red\")\n",
    "    plt.annotate(sentence, (sentence_embeddings_2d[i, 0], sentence_embeddings_2d[i, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Run the cell below to submit your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your quiz answers (run this cell to submit)\n",
    "\n",
    "quizdt.store(\n",
    "    dict(\n",
    "        notebook=\"W9D1_Text_Embeddings\",\n",
    "        my_pennkey=my_pennkey,\n",
    "        my_pod=my_pod,\n",
    "        my_email=my_email,\n",
    "        why_ohe_pca_isnt_useful=why_ohe_pca_isnt_useful,\n",
    "        how_fabricated_embeddings=how_fabricated_embeddings,\n",
    "        sentence_vectorization=sentence_vectorization\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
