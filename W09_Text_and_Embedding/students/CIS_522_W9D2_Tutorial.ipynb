{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U datatops==0.2.2 gensim==4.3.1 matplotlib==3.7.1 matplotlib-inline==0.1.3 numpy==1.24.2 requests==2.28.2 requests-oauthlib==1.3.0 scikit-learn==1.2.2 vibecheck==0.0.3 pytorch_lightning torch tqdm ipywidgets datasets transformers typing_extensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers: Preface\n",
    "\n",
    "* **Authors**: Jordan Matelsky\n",
    "* **Reviewers**: Lyle Ungar, Konrad Kording\n",
    "\n",
    "**A GPU is recommended for this notebook.**\n",
    "\n",
    "> * This notebook will use a new feedback mechanism to get vibe-checks throughout the notebook; please do feel encouraged to click the feedback buttons to submit anonymous feedback! This will not count toward your grade.\n",
    "> * In general, Jupyter notebooks should follow good code-style practices, and all the imports should go at the top. But because much of this code might be useful to you in the future, we will leave cells with imports in them throughout the notebook so that it is easy to copy and paste to reuse this code. For more information on notebook best practices, see [here](https://gist.github.com/j6k4m8/864fb5a8a2257237169a48b6bd6a307e).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback setup (run this cell)\n",
    "\n",
    "# Feedback with Datatops\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from datatops import Datatops\n",
    "\n",
    "feedback_dtid = \"62a48t3w\"\n",
    "feedback_name = \"cis522_feedback\"\n",
    "quiz_dtid = \"lxx8szk1\"\n",
    "quiz_name = \"cis522_quiz\"\n",
    "notebook_name = \"W9D2_Tokenizers\"\n",
    "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
    "\n",
    "# Instantiate the Datatops client\n",
    "dt = Datatops(dt_url)\n",
    "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pennkey = \"\" #@param {type:\"string\"}\n",
    "my_pod = \"\" #@param {type:\"string\"}\n",
    "my_email = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "In the previous notebook, we discussed a few ways to represent words as vectors: We generated one-hot encodings of words in a vocabulary, and saw that it lacked the ability to cluster similar words. We then generated context-free embeddings, and saw that they clustered words together but could not distinguish different meanings of the same word (for example, homonyms or alternate uses). Today, we will complete our triptych of embedding techniques, and continue our march toward large language models and transformers by discussing one of the critical developments of the modern NLP stack: **Tokenization.**\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "* Write a tokenizer from scratch, taking advantage of context. Play with a few pre-trained tokenizers from industry.\n",
    "* Discuss RNNs and LSTMs\n",
    "* Advanced discussions in neural memory\n",
    "* Deep-dive discussion of vanishing gradients with long sequences\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generating a dataset\n",
    "\n",
    "We wrote our own embedders as an academic exercise; as we continue to move closer to \"production-grade\" NLP, we'll start to use industry standards such as the [HuggingFace](https://huggingface.co/) library. \n",
    "\n",
    "We'll start by generating a training dataset (like we used Moby Dick in the previous day's notebooks). `hf` has a convenient `datasets` module that allows us to download a variety of datasets, including the [Wikipedia text corpus](https://huggingface.co/datasets/wiki_text). We'll use this to generate a dataset of text from Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[41492])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_examples(dataset, n=512):\n",
    "    \"\"\"\n",
    "    Produce a generator that yields n examples at a time from the dataset.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), n):\n",
    "        yield dataset[i:i + n]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the actual `Tokenizer`, adhering to the [`hf.Tokenizer` protocol](https://huggingface.co/docs/transformers/main_classes/tokenizer). (Much like we did last time by adhering to a standard protocol, this enables us to swap in our tokenizer for any tokenizer in the huggingface ecosystem, or to apply our own tokenizer to any model in the huggingface ecosystem.) \n",
    "\n",
    "Let's sketch out the steps of writing a Tokenizer. We need to solve two problems:\n",
    "\n",
    "* Given a string, split it into a list of tokens.\n",
    "* If you don't recognize a word, still figure out a way to tokenize it!\n",
    "\n",
    "This may feel like we're reinventing our one-hot encoder from the previous notebook, but with a richer vocabulary. Why is it that the OHE, which output a vector of length `|V|`, where `|V|` is the size of our vocabulary, is not sufficient, but a tokenizer that outputs a list of indices into a vocabulary of size `|V|` is sufficient? The answer is that while our encoder was responsible for embedding words into a high-dimensional space, our tokenizer is NOT; the \"win\" of a tokenizer is that it breaks up a string into in-vocab elements. For certain workflows, the very next step might be adding an embedder onto the end of the tokenizer. (As we'll soon see, this is exactly the strategy employed by modern Transformer models.) \n",
    "\n",
    "Tokens will almost always be different from words; for example, we might want to split \"don't\" into \"do\" and \"n't\", or we might want to split \"don't\" into \"do\" and \"not\". Or we might even want to split \"don't\" into \"d\", \"o\", \"n\", and \"t\". We can choose any strategy we want here; **unlike Word2Vec, our tokenizer will NOT be limited to outputting one vector per English word.** Here, we'll use an off-the-shelf subword splitter, which we discuss below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try playing with these hyperparameters!\n",
    "VOCAB_SIZE = 12_000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, Regex, models, normalizers, pre_tokenizers, trainers, processors\n",
    "\n",
    "# Create a tokenizer object that uses the \"WordPiece\" model. The WorkPiece model\n",
    "# is a subword tokenizer that uses a vocabulary of common words and word pieces\n",
    "# to tokenize text. The \"unk_token\" parameter specifies the token to use for\n",
    "# unknown tokens, i.e. tokens that are not in the vocabulary. (Remember that the\n",
    "# vocabulary will be built from our dataset, so it will include subchunks of \n",
    "# English words.)\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Features\n",
    "\n",
    "Now let's start dressing up our tokenizer with some useful features. First, let's clean up the text. This process is formally called \"normalization\", and it is a critical step in any NLP pipeline. We'll start by removing punctuation, and then we'll convert all the text to lowercase. We'll also remove diacritics (accents) from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of a Normalizer Sequence the same way you would think of a PyTorch\n",
    "# Sequential model. It is a sequence of normalizers that are applied to the\n",
    "# text before tokenization, in the order that they are added to the sequence.\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(Regex(r\"[\\s]\"), \" \"), # Convert all whitespace to single space\n",
    "    normalizers.Lowercase(), # Convert all text to lowercase\n",
    "    normalizers.NFD(), # Decompose all characters into their base characters\n",
    "    normalizers.StripAccents(), # Remove all accents\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add a pre-tokenizer. The pre-tokenizer is applied to the text after we normalize it, but before it's tokenized. The pre-tokenizer is useful for splitting text into chunks that are easier to tokenize. For example, we can split text into chunks that are separated by punctuation or whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(), # Split on whitespace\n",
    "    pre_tokenizers.Digits(individual_digits=True), # Split digits into individual tokens\n",
    "    pre_tokenizers.Punctuation(), # Split punctuation into individual tokens\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll train the tokenizer with our dataset. There are a few different algorithms for training tokenizers. Here are two common ones:\n",
    "\n",
    "* BPE Algorithm: Start with a vocabulary of each character in the dataset. Examine all pairs from the vocabulary and merge the pair with the highest frequency in the dataset. Repeat until the vocabulary size is reached. (So \"ee\" is more likely to get merged than \"zf\" in the english corpus)\n",
    "* Top-Down WordPiece Algorithm: Generate all substrings of each word from the dataset and count occurrences in the training data. Keep any string that occurs more than a threshold number of times. Repeat this process until the vocabulary size is reached. (For a more thorough explanation of this process, see [the TensorFlow Guide](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm).)\n",
    "\n",
    "We'll use WordPiece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    # We have to specify the special tokens that we want to use. These will be\n",
    "    # added to the vocabulary no matter what the vocab-building algorithm does.\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those special tokens are important because it tells the WordPiece training process how to treat phrases, masks, and unknown tokens. (Note that we can also add our own special tokens, such as `[CITE]` to indicate when a citation is about to be used, if we wanted to train a model to predict the presence of citations in a text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_n_examples(dataset), trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In \"real life\", we'd probably want to save the tokenizer to disk so that we\n",
    "# can use it later. We can do this with the \"save\" method:\n",
    "# tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Let's try it out!\n",
    "print(\"Hello, world!\")\n",
    "print(\n",
    "    *zip(\n",
    "        tokenizer.encode(\"Hello, world!\").tokens,\n",
    "        tokenizer.encode(\"Hello, world!\").ids,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Can we also tokenize made-up words?\n",
    "print(tokenizer.encode(\"These toastersocks are so groommpy!\").tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The `##` means that the token is a continuation of the previous chunk.)\n",
    "\n",
    "Try playing around with the hyperparameters and the tokenizing algorithms to see how they affect the tokenizer's output. There can be some very major differences!\n",
    "\n",
    "In summary, we created a tokenizer pipeline that:\n",
    "\n",
    "* Normalizes the text (cleans up punctuation and diacritics)\n",
    "* Splits the text into chunks (using whitespace and punctuation)\n",
    "* Trains the tokenizer on the dataset (using the WordPiece algorithm)\n",
    "\n",
    "In common use, this would be the first step of any modern NLP pipeline. The next step would be to add an embedder to the end of the tokenizer, which we'll do shortly, so that we can feed in a high-dimensional space to our model. But unlike Word2Vec, we can now separate the tokenization step from the embedding step, which means our encoding/embedding process can be task-specific, custom to our downstream neural net architecture, instead of general-purpose.\n",
    "\n",
    "\n",
    "## Quiz!\n",
    "\n",
    "**We established that the tokenizer is a better move than the OHE because it can handle out-of-vocabulary words. But what if we just made a one-hot encoding where the vocabulary is all possible two-character combinations? Would there still be an advantage to the tokenizer?** (Hint: Re-read the section on the BPE and WordPiece algorithms, and how the tokens are selected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vs_combinatorial_ohe = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's think about a language like Chinese, where words are each composed of a relatively fewer number of characters compared to English (`hungry` is six unicode characters, but `饿` is one unicode character), but there are many more unique Chinese characters than there are letters in the English alphabet. In a one or two sentence high-level sketch, what properties would be desireable for a Chinese tokenizer to have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_chinese_tokenizer_properties = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W9D2_Tokenizers\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Neural Networks\n",
    "\n",
    "This week, we've gone deep on the theory of how to cram natural language — a squishy, fickle thing — into a neural network, a machine that expects numerical and quantifiable inputs. In the next section, we'll start to see how we can use these resources to build powerful natural language processing models.\n",
    "\n",
    "First, let's motivate things with an example. Autocomplete is a common feature in nearly every text-editing experience these days; the task is quite simple. Given a string of text, we want the machine to predict the next word.\n",
    "\n",
    "Let's rewind our brains to 2010; how might we have solved this task without a neural network? One obvious answer is a Markov chain. A Markov process is a state machine that transitions from one state to another based on the current state and a probability distribution. For example, consider the following diagram:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNp9j7EOgkAMhl_l0tEAQROXG5z0CRw5h4YregnHmdIzMYR3twiudur_5WvaTtAmT2BhFBQ6B7wzxvJ1cIPRanY3U5YnIw-yZl_VK9X0pTGx4ro6rniJ_2xiDsTbABSgIGLwuntaTAdqRnJgtfXUYe7FgRtmVfPT63UXHyQx2A77kQrALOn6Hlqwwpl-0vbCZs0f2qFFcA?type=png)](https://mermaid.live/edit#pako:eNp9j7EOgkAMhl_l0tEAQROXG5z0CRw5h4YregnHmdIzMYR3twiudur_5WvaTtAmT2BhFBQ6B7wzxvJ1cIPRanY3U5YnIw-yZl_VK9X0pTGx4ro6rniJ_2xiDsTbABSgIGLwuntaTAdqRnJgtfXUYe7FgRtmVfPT63UXHyQx2A77kQrALOn6Hlqwwpl-0vbCZs0f2qFFcA)\n",
    "\n",
    "The sentence \"the more the merrier\" has been encoded here as a Markov process. If you are at state \"the\", you have a 50% chance of transitioning to \"more\", and a 50% chance of transitioning to \"merrier\". If you are at state \"more\", you have a 100% chance of transitioning to \"the\". \n",
    "\n",
    "It is very easy to construct a Markov model; simply walk along your input corpus and count the number of times each word follows another word. Then, normalize the counts to get a probability distribution.  And it's very easy to reconstruct a sequence from a Markov model; just walk along the graph, and pick the next state change based on the probability distribution of your current state.\n",
    "\n",
    "One issue with this approach is that it doesn't take into account the context of the word. For example, sitting at the node \"the\" in the network above, we could fall into the \"more\" state over and over:\n",
    "\n",
    "```\n",
    "the more the more the more the more the merrier\n",
    "```\n",
    "\n",
    "In this case, more _more_ is NOT merrier :)\n",
    "\n",
    "A Markov model is a first-order Markov process, which means that it only considers the current state. A second-order Markov process considers the current state and the previous state. It's easy to create a second-order Markov process by adding $n$-grams instead of single words:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNp1kMsKwjAQRX8lzEpLWqrgJgtX-gVus5k20xpoG0lTRUr_3Sl9YBGzupw5YYbbQ-4MgYI2YKCLxdJjHT-PQjeCXyTi-Cx2kQx32itxSNKJz2SacpK186OQJqc_QmbLzXxhk8FJZmg2KxY2G2jky1XFRlk3T84Yfy5d4dex5L0lv2gggUmN1nAP_fhNA2s1aVAcDRXYVUGDbgZWu4fhpq7GBudBFVi1JAG74G7vJgcVfEeLNNc5W8MHuudpsQ?type=png)](https://mermaid.live/edit#pako:eNp1kMsKwjAQRX8lzEpLWqrgJgtX-gVus5k20xpoG0lTRUr_3Sl9YBGzupw5YYbbQ-4MgYI2YKCLxdJjHT-PQjeCXyTi-Cx2kQx32itxSNKJz2SacpK186OQJqc_QmbLzXxhk8FJZmg2KxY2G2jky1XFRlk3T84Yfy5d4dex5L0lv2gggUmN1nAP_fhNA2s1aVAcDRXYVUGDbgZWu4fhpq7GBudBFVi1JAG74G7vJgcVfEeLNNc5W8MHuudpsQ)\n",
    "\n",
    "An unfortunate consequence is an explosion in the memory requirements of the model, which goes from $O(n)$ to $O(n^2)$. So we have to make a tradeoff; a 10-gram model will be totally impossible to fit into memory, but a simple first-order model will be too simplistic to capture the context of the words.\n",
    "\n",
    "Trying to solve this problem — the context of older words in the sequence \"vanishing\" from the model's awareness — will lead to one of the most critical inflection points in machine learning history (and, if you're feeling generous, perhaps one of the greatest turning points in human history). Let's explore why this is such a big problem to begin with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Iterable, List\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class MarkovModel:\n",
    "\n",
    "    def __init__(self, order, tokenizer: Tokenizer):\n",
    "        self.order = order\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = {}\n",
    "\n",
    "    def fit(self, text_loader: Iterable[str], iters: int = 50):\n",
    "        \"\"\"\n",
    "        Fit the Markov model to the text.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        for i, text in tqdm(enumerate(text_loader), total=iters):\n",
    "            if i > iters:\n",
    "                break\n",
    "            tokens = self.tokenizer.encode(\" \".join(text)).tokens\n",
    "            self._fit_tokens(tokens)\n",
    "\n",
    "    def _fit_tokens(self, tokens: List[str]):\n",
    "        # Iterate over the tokens, and for each token, add the next token to the\n",
    "        # model. The key is the previous \"order\" tokens, and the value is a list\n",
    "        # of the next tokens.\n",
    "        for i in range(len(tokens) - self.order):\n",
    "            key = tuple(tokens[i:i + self.order])\n",
    "            value = tokens[i + self.order]\n",
    "            if key not in self.model:\n",
    "                self.model[key] = []\n",
    "            self.model[key].append(value)\n",
    "\n",
    "    def generate(self, n=100, prefix: str = None):\n",
    "        \"\"\"\n",
    "        Generate a sequence of tokens using the Markov model.\n",
    "        \"\"\"\n",
    "        # If a prefix is specified, use it to initialize the sequence. Otherwise,\n",
    "        # choose a random key from the model.\n",
    "        if prefix is not None:\n",
    "            sequence = self.tokenizer.encode(prefix).tokens\n",
    "            key = tuple(sequence[-self.order:])\n",
    "        else:\n",
    "            key = random.choice(list(self.model.keys()))\n",
    "\n",
    "        # Initialize the sequence with the key\n",
    "        sequence = list(key)\n",
    "        \n",
    "        # TODO: Populate sequence incrementally\n",
    "        raise NotImplementedError(\"TODO: Populate sequence incrementally\")\n",
    "    \n",
    "        return (f\"{prefix}\" if prefix else \"\") + \"\".join([f\"{w[2:]}\" if w.startswith(\"##\") else f\" {w}\" for w in sequence[self.order:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Markov model. Try changing the order, number of training iters, \n",
    "# and the prefix!\n",
    "markov_model = MarkovModel(3, tokenizer)\n",
    "\n",
    "# Fit the model to the dataset\n",
    "markov_model.fit(generate_n_examples(dataset), iters=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model.generate(50, prefix=\"the first person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title .\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W9D2_Markov\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "If you played a bunch with the Markov model above, you might have noticed that sometimes you give it a prefix that it can't complete, because it encounters a token or set of tokens that it hasn't seen before. That means that the probability distribution of the next token is undefined! \n",
    "\n",
    "What would be really nice is to have some sort of fuzzy transition probability. For example, maybe you haven't seen the exact string before, but you've seen something similar. It would be nice if each hop on the graph had a _neural_ transition probability map!\n",
    "\n",
    "Recurrent neural networks (RNNs) were a class of neural networks that had this property. The way a recurrent network works is you feed it the first windowed chunk of an input; it creates an output, and then you feed the NEXT part of the input, along with that output, to the network _again_. In other words, the RNN gets the new input, and it also learns to interpret its own output from the previous window:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/693511/226206101-b0001f12-5375-4708-a3a6-4c03541ef8a5.png\">\n",
    "\n",
    "RNNs mix all the previous outputs into the current input; so you get a mixture of the previous output vector alongside the next bit of the sequence. In this way, you get the \"n-back\" property of the Markov chains, but with the flexibility and power of neural networks.\n",
    "\n",
    "### The Vanishing Gradient Problem\n",
    " \n",
    "There's a major problem though; each time you combine the previous outputs, you dilute the importance of those predecessor vectors a bit. Your humble author envisions this akin to mixing colors of paint: Each time you run the next set of inputs through the RNN, you mix one gallon of the new color with one gallon of the old (combined) color. After only a few iterations, the starting color of the paint no longer matters because it's been diluted so much by new inputs. Likewise, after only a few iterations, the previous sequence elements no longer matter because they've been diluted by the new elements.\n",
    "\n",
    "This phenomenon is called the _vanishing gradients_ problem, and it was a major stumbling block for RNNs. Many solutions have been attempted to fix this forgetfulness problem, but the most popular one is the _long short-term memory_ (LSTM) network. LSTMs are a special kind of RNN that have a special kind of memory cell that can be \"activated\" or \"deactivated\" at will. This allows the network to selectively remember or forget information as it sees fit. But even LSTMs suffer from vanishing gradients, though they can usually hold out a little longer and remember information for a few more iterations.\n",
    "\n",
    "Below the submission cell, we have implemented an LSTM network, and you can play with its hyperparameters to experiment with this style of network if you like! Take note of how we generate text, recursively calling the same model with progressive inputs. Don't worry if you don't understand all the code; ultimately we will throw it all away this weekend and start with a new model on Monday — the model class that has revolutionized modern deep learning and changed the world.\n",
    "\n",
    "## Quiz:\n",
    "\n",
    "**You are training a RNN at BigCorp Inc. The deadline for your project is tomorrow, and a coworker suggests that you train your LSTM in parallel, training early parts of the sequence and later parts of the sequence simultaneously. What is the problem with this approach?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_rnn_parallelism_breaks = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your quiz answers (run this cell to submit)\n",
    "\n",
    "quizdt.store(\n",
    "    dict(\n",
    "        notebook=notebook_name,\n",
    "        my_pennkey=my_pennkey,\n",
    "        my_pod=my_pod,\n",
    "        my_email=my_email,\n",
    "        tokenizer_vs_combinatorial_ohe=tokenizer_vs_combinatorial_ohe,\n",
    "        useful_chinese_tokenizer_properties=useful_chinese_tokenizer_properties,\n",
    "        why_rnn_parallelism_breaks=why_rnn_parallelism_breaks,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory:\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest possible implementation of an RNN:\n",
    "from typing import Dict\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class AutocompleteRNN(pl.LightningModule):\n",
    "    def __init__(self, tokenizer: Tokenizer, encoder_size=128, decoder_size=128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.encoder = nn.Embedding(tokenizer.get_vocab_size(), encoder_size)\n",
    "        self.decoder = nn.Linear(decoder_size, tokenizer.get_vocab_size())\n",
    "        self.rnn = nn.GRU(encoder_size, decoder_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack the batch, which is shape (batch, (x, y)))\n",
    "        batches = batch\n",
    "        for batch in batches:\n",
    "            x, y = batch\n",
    "            y_hat = self(x)\n",
    "            loss = F.cross_entropy(y_hat.view(-1, y_hat.shape[-1]), y.view(-1))\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, y_hat.shape[-1]), y.view(-1))\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dset = AutocompleteDataset(self.tokenizer, generate_n_examples(dataset), 100)\n",
    "        return DataLoader(dset, batch_size=32, shuffle=True, collate_fn=dset.collate_fn)\n",
    "\n",
    "    \n",
    "    \n",
    "class AutocompleteDataset(Dataset):\n",
    "    def __init__(self, tokenizer: Tokenizer, examples: Iterable[Dict[str, List[str]]] , max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = list(examples)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = \" \".join(self.examples[idx])\n",
    "        tokens = self.tokenizer.encode(example).ids\n",
    "        # Pad the sequence to the max length:\n",
    "        x = tokens[:-1]\n",
    "        y = tokens[1:]\n",
    "        x = torch.tensor(x + [0] * (self.max_len - len(x)))\n",
    "        y = torch.tensor(y + [0] * (self.max_len - len(y)))\n",
    "        return x, y\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        # Pad both x and y to the max length in the batch:\n",
    "        max_len = max([len(b[0]) for b in batch])\n",
    "        for i in range(len(batch)):\n",
    "            x, y = batch[i]\n",
    "            # concat with padding:\n",
    "            x = torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)])\n",
    "            y = torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)])\n",
    "            batch[i] = (x, y)\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "    \n",
    "model = AutocompleteRNN(tokenizer)\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator=\"gpu\" if torch.cuda.is_available() else 'cpu')\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.cpu()\n",
    "prompt = \"This is a\"\n",
    "outputs = []\n",
    "\n",
    "for i in range(10):\n",
    "    tokens = tokenizer.encode(prompt).ids\n",
    "    x = torch.tensor(tokens + [0] * (100 - len(tokens)))\n",
    "    y_hat = model(x.unsqueeze(0))\n",
    "    y_hat = y_hat[0, len(tokens) - 1]\n",
    "    y_hat = y_hat.argmax(-1)\n",
    "    y_hat = y_hat.item()\n",
    "    prompt += \" \" + tokenizer.decode([y_hat])\n",
    "\n",
    "    outputs.append(y_hat)\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
