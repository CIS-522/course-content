{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Images with Diffusion \n",
    "\n",
    "* **Content Creator:** Jordan Matelsky\n",
    "* **Content Reviewers:** Konrad Kording, Lyle Ungar\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Unless you've been living under a rock, you've probably seen some pretty extraordinary generative AI in the past year. Among the most impressive is *stable diffusion*, an architecture that can generate images that are indistinguishable from \"real\" images. In this notebook, we'll take a look at how stable diffusion works, and see why it's so powerful. \n",
    "\n",
    "## Why aren't we covering GANs?\n",
    "\n",
    "Generative adversarial networks (GANs) were the state-of-the-art for AI-generated images for a long time, but they have some serious drawbacks. If you've ever trained a network before and then retrained it only to get completely different performance, then you know how fragile some network architectures can be. Because GANs involve training a generator and a discriminator network in parallel, they're notoriously unstable, and can easily get stuck with one model \"winning\" over the other. They can also be difficult to steer in order to tailor the outputs — something we didn't realize as a community until we saw how overwhelmingly EASY it is to steer newer architectures, like stable diffusion. \n",
    "\n",
    "Originally we weren't even going to mention them in the course, but your beloved TA Jordan has a soft-spot for GANs since they were the first \"big boy\" networks he ever trained. So here you go, GANs — this paragraph is an ode to you. Now, onward to better and more stable things!\n",
    "\n",
    "## GPU Runtime!\n",
    "\n",
    "We won't build terribly large models today, but you WILL want to use a GPU runtime. If you're using Colab, you can change the runtime type in the menu above. If you're using Jupyter, you can use the `nvidia-smi` command to check if you have a GPU available.\n",
    "\n",
    "## Today's Gameplan\n",
    "\n",
    "- Understand dimensionality-reduction as a denoiser\n",
    "- Learn how latent diffusion models work, and what the difference is between stable diffusion and latent diffusion\n",
    "- Build our own simple latent-diffusion toy from scratch so we can touch its guts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pennkey = \"\" #@param {type:\"string\"}\n",
    "my_pod = \"\" #@param {type:\"string\"}\n",
    "my_email = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U matplotlib scikit-image numpy torch torchvision tqdm pytorch_lightning scikit-learn vibecheck datatops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback setup (run this cell) { display-mode: \"form\" }\n",
    "\n",
    "# Feedback with Datatops\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from datatops import Datatops\n",
    "\n",
    "feedback_dtid = \"62a48t3w\"\n",
    "feedback_name = \"cis522_feedback\"\n",
    "quiz_dtid = \"lxx8szk1\"\n",
    "quiz_name = \"cis522_quiz\"\n",
    "dt_url = \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab/\"\n",
    "\n",
    "# Instantiate the Datatops client\n",
    "dt = Datatops(dt_url)\n",
    "quizdt = dt.get_project(quiz_name, user_key=quiz_dtid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is diffusion?\n",
    "\n",
    "Place a massless particle at a point in space, and then repeatedly add or subtract a random small amount to its position, sampling $d \\sim \\mathcal{N}(0, 1)$. The particle will follow the laws of [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion). (Fun fact that will make you fun to talk to at parties: Before Einstein worked on relativity, he studied [the physics of diffusion](https://en.wikipedia.org/wiki/Brownian_motion#Einstein's_theory).)\n",
    "\n",
    "Instead of moving a particle in 3D (and sampling $(d_x, d_y, d_z)$), we can move a particle in 784 dimensions. (Bear with me for a second.) Let's start at the origin, and see what happens over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play with hyperparameters! Note hyperparameters may be hardware-dependent.\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 784\n",
    "iters = 1000\n",
    "all_vecs = np.zeros((iters, dim))\n",
    "for i in range(1, iters):\n",
    "    vec = np.random.normal(loc=0, scale=1, size=dim)\n",
    "    all_vecs[i] = all_vecs[i-1] + vec\n",
    "\n",
    "plt.imshow(all_vecs, cmap='magma')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty! But what does it mean? Well, we can think of each dimension as a pixel in an image. So, if we start at the origin, and then diffuse one pixel/dimension at a time, we can \"diffuse\" an entire image. (Note that in this example, we diffuse each pixel independently!) In this interpretation, there is no meaning to the shape of the image; you can imagine it as a reshaped vector, that just happens to be a rectangle for visualization purposes.\n",
    "\n",
    "Let's try it, but instead of starting at the origin, we're going to start at a specially chosen point in the space — specifically, the point that corresponds to this picture of a cat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "\n",
    "iters = 10\n",
    "plt.subplots(1, iters, figsize=(10, 2), dpi=150)\n",
    "plt.subplot(1, iters, 1)\n",
    "last_img = data.cat().astype(np.float64)\n",
    "plt.imshow(last_img / 255)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "for i in range(2, iters+1):\n",
    "    plt.subplot(1, iters, i)\n",
    "    last_img += np.random.normal(loc=0, scale=255, size=last_img.shape)\n",
    "    plt.imshow(np.clip(last_img, 0, 255) / 255)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.suptitle(\"The great mystery of the disappearing cat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image on the left is the original image, and the images on the right are the original image with random noise cumulatively added to it. \n",
    "\n",
    "**The main idea of a diffusion model is to \"learn\" the inversion of this process**, so that given the image on the far right, we can incrementally de-noise it to get back to the image on the left. Of course, _actually_ inverting this process is a hard (likely impossible!) task.\n",
    "\n",
    "Let's look at how we might go about performing this denoising process. Can you think of any operations we've done in the past that had the effect of denoising images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title How did this section feel? { display-mode: \"form\" }\n",
    "#@markdown This is a new feedback format we are trying out. You will see these prompts throughout the notebook. Please let us know your candid (anonymous) feedback! This will not be associated with your quiz answers.\n",
    "\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W8D2_Diffusion_Explanation\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dimensionality Reduction Can Be a Denoising Process\n",
    "\n",
    "One way to denoise an image is to reduce its dimensionality using a technique that preserves \"important\" information and disregards \"unimportant\" information. For example, we can use a [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) to reduce the dimensionality of an image. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "# Load the faces data from the LFW dataset\n",
    "faces = fetch_olivetti_faces()\n",
    "X = faces.data\n",
    "\n",
    "# Create a PCA model with 100 components\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X)\n",
    "\n",
    "# Project the data onto the first two principal components\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Plot the first 10 eigenfaces\n",
    "plt.subplots(1, 10, figsize=(20, 2), dpi=150)\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(pca.components_[i].reshape(64, 64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we took the principal components of the face dataset distribution and used them to reduce the amount of information we need to store about each face. For example, instead of storing the whole image, we can now store (1) the components, and (2) just 100 measly scalars, and still get a pretty good approximation of the original image. And because PCA chooses which components are the most important to the structure of the image, it definitionally throws away \"noise\" — the dimensions of the image that don't matter much to its overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first face from the dataset as the original image,\n",
    "# the first reconstructed face,\n",
    "# and a modified version of the face by manipulating the \"latent\" vector\n",
    "plt.subplots(1, 3, figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X[0].reshape(64, 64), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(pca.inverse_transform(X_pca)[0].reshape(64, 64), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reconstructed\")\n",
    "# Synthetic!\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(\n",
    "    pca.inverse_transform(X_pca + np.random.normal(0, 0.5, size=X_pca.shape))[0].reshape(\n",
    "        64, 64\n",
    "    ),\n",
    "    cmap=\"gray\",\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Modified\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot three faces: First, a face from the dataset, then the closest we can get to that face using only the parameters stored in the model (i.e., the principal components), and finally, **a face that we've generated using a made-up vector.** \n",
    "\n",
    "That's a neat trick... let's do it again, using a totally made up vector. To be cheeky, let's use the 25-character string \n",
    "\n",
    "> this person doesn't exist\n",
    "\n",
    "repeated four times, in ascii:\n",
    "\n",
    "```\n",
    "116 104 105 115 32 112 101 ...\n",
    "T   H   I   S   _   P   E  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector = np.array([\n",
    "    116,104,105,115,32,112,101,114,115,111,110,32,100,111,101,115,110,39,116,32,101,120,105,115,116,\n",
    "] * 4)\n",
    "\n",
    "# Reverse the PCA transformation on this made-up vector\n",
    "# (downscaled to get things into a reasonable range)\n",
    "reconstructed = pca.inverse_transform([my_vector * 0.001])\n",
    "\n",
    "# Plot the reconstructed face\n",
    "plt.imshow(reconstructed.reshape(64, 64), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz time! We just boldly declared that PCA can be thought of as a denoising model. What are some reasons why we couldn't use PCA as our denoiser in the latent diffusion model?**\n",
    "\n",
    "Hint: ChatGPT doesn't know about stable diffusion yet, so you're on your own here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_pca_isnt_a_good_denoiser = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural denoising\n",
    "\n",
    "There are some clear disadvantages to PCA as a denoiser, chief among them that it is a linear model. This means that it can only learn to denoise images by performing linear combinations of the principal components. This is a pretty big limitation, and it's one we can overcome by using a neural network instead of a linear model.\n",
    "\n",
    "There are a few architectures that we could use to denoise images using the same dimensionality-reduction trick as PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings for our neural networks:\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "IMG_SIZE = 64\n",
    "IMG_SIZE_PROD = IMG_SIZE * IMG_SIZE\n",
    "IMG_CHANNELS = 1\n",
    "CHOKEPOINT_SIZE = 256\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Dimensionality Reduction with Autoencoders\n",
    "\n",
    "We can also use an autoencoder to reduce the dimensionality of an image. An autoencoder is a neural network that takes an image as input, and then tries to reconstruct that same image as output. The network is trained to minimize the reconstruction error, which is the difference between the input and the output. Seems easy, right? The tricky part is that the network has to move all of the data through a chokepoint; a layer with very few units. This chokepoint layer, and the network that leads up to it, is called the **encoder**. The network that leads away from the chokepoint is called the **decoder**.\n",
    "\n",
    "In the example below, we'll write a very simple autoencoder for the face dataset. The encoder will take an image as input, and then output a vector of length 100. The decoder will take that vector as input, and then output an image. The network will be trained to minimize the reconstruction error, which is the difference between the input image and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch Lightning to train a simple autoencoder.\n",
    "# Some code borrowed from https://pytorch-lightning.readthedocs.io/en/latest/model/train_model_basic.html\n",
    "\n",
    "\n",
    "class Encoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            IMG_CHANNELS, 32, kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        # Linear:\n",
    "        self.fc1 = torch.nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, CHOKEPOINT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convs + ReLU, three times:\n",
    "        raise NotImplementedError(\"Implement three blocks of Convs + ReLU\")\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # unlinear:\n",
    "        self.fc1 = torch.nn.Linear(CHOKEPOINT_SIZE, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 128 * 8 * 8)\n",
    "        # unflatten:\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        raise NotImplementedError(\"What should the last convolutionTranspose be? (Hint: the output needs to look like... an image!)\")\n",
    "        self.conv3 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1, 128, 8, 8)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(-1, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "        enc = self.encoder(x)\n",
    "        x_hat = self.decoder(enc)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "faces = fetch_olivetti_faces()\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(faces.data).float(), torch.from_numpy(faces.target).long()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "# Bonus pro-tips: PyTorch Lightning is a super useful tool!\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=20,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    ")\n",
    "trainer.fit(model=autoencoder, train_dataloaders=dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first face from the dataset as the original image,\n",
    "# the first reconstructed face,\n",
    "# and a modified version of the face by manipulating the \"latent\" vector\n",
    "plt.subplots(1, 3, figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(faces.data[0].reshape(64, 64), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.subplot(1, 3, 2)\n",
    "# Reconstruct the first face from the dataset:\n",
    "latent = autoencoder.encoder(torch.from_numpy(faces.data[0].reshape(1, 1, 64, 64)).float())\n",
    "reconstructed = autoencoder.decoder(latent)\n",
    "plt.imshow(reconstructed.detach().numpy().reshape(64, 64), cmap=\"gray\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.subplot(1, 3, 3)\n",
    "# A completely made-up face, with a random latent:\n",
    "latent_new = torch.randn(1, CHOKEPOINT_SIZE)\n",
    "reconstructed_new = autoencoder.decoder(latent_new)\n",
    "plt.imshow(reconstructed_new.detach().numpy().reshape(64, 64), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Totally New\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...alright, I mean... they're not the *prettiest* looking faces. But we're able to reconstruct a face surprisingly well, considering that ALL we're using is random numbers and the weights of our model. \n",
    "\n",
    "Try experimenting with the number of units in the chokepoint layer, the learning rate, the batch size... see if you can get better results! Can you find parameters that convince yourself that the reconstructed face in panel 2 is \"just as good\" as the original face in panel 1?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Dimensionality Reduction with a U-Net\n",
    "\n",
    "A U-net neural network is another architecture that \"downsamples\" an image into a lower dimensional latent representation, and then \"upsamples\" it back to the original image. The network is trained to minimize the reconstruction error, just like the autoencoder. \n",
    "\n",
    "The main difference is that the U-net architecture \"shares\" information between the encoder and the decoder. This has some advantages and disadvantages. The main advantage is that U-nets' decoders get more of the original input's \"global\" information, because some of the information reaching the decoder is preserved from the encoder. The main _disadvantage_, as far as we're concerned, is that because of these skip connections, we can't really inject a latent into the chokepoint of the U-net, because we'd also need to invent realistic information for the skip connections. And unlike the autoencoder, we can't just use random numbers for the skip connections, because the skip connections are supposed to be meaningful global information about the input:\n",
    "\n",
    "![A comparison of U Nets and Autoencoders](https://user-images.githubusercontent.com/693511/216820060-44c33768-2e98-48af-b746-046046681cf7.png)\n",
    "\n",
    "So unfortunately, it's much harder to use a U-net to create new synthetic outputs. But it's still a very powerful denoiser, so let's review the three tools we have so far:\n",
    "\n",
    "* PCA: An okay denoiser, but is \"merely\" a linear model.\n",
    "* Autoencoders: Decent denoiser, and can be used to inject new latent vectors into the chokepoint to make synthetic images.\n",
    "* U-Nets: Very good denoiser, but can't practically be used to inject latent vectors into the chokepoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://github.com/gregbugaj/unet-denoiser\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up, self).__init__()\n",
    "        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x2 = self.up_scale(x2)\n",
    "\n",
    "        diffY = x1.size()[2] - x2.size()[2]\n",
    "        diffX = x1.size()[3] - x2.size()[3]\n",
    "\n",
    "        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down_layer, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pool(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_layer, self).__init__()\n",
    "        self.up = up(in_ch, out_ch)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        a = self.up(x1, x2)\n",
    "        x = self.conv(a)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, dimensions=2):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = double_conv(1, 64)\n",
    "        self.down1 = down_layer(64, 128)\n",
    "        self.down2 = down_layer(128, 256)\n",
    "        self.down3 = down_layer(256, 512)\n",
    "        self.down4 = down_layer(512, 1024)\n",
    "        self.up1 = up_layer(1024, 512)\n",
    "        self.up2 = up_layer(512, 256)\n",
    "        self.up3 = up_layer(256, 128)\n",
    "        self.up4 = up_layer(128, 64)\n",
    "        self.last_conv = nn.Conv2d(64, dimensions, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to batch x w x h:\n",
    "        x = x.reshape(-1, 1, 64, 64)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x1_up = self.up1(x4, x5)\n",
    "        x2_up = self.up2(x3, x1_up)\n",
    "        x3_up = self.up3(x2, x2_up)\n",
    "        x4_up = self.up4(x1, x3_up)\n",
    "        output = self.last_conv(x4_up)\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, x.reshape(-1, 1, 64, 64))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "faces = fetch_olivetti_faces()\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(faces.data).float(), torch.from_numpy(faces.target).long()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "model = UNet()\n",
    "# Initialize the model:\n",
    "x, y = next(iter(dataloader))\n",
    "model(x)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, accelerator=\"gpu\" if torch.cuda.is_available() else None)\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first item from the dataset:\n",
    "example_face = faces.data[0].reshape(64, 64)\n",
    "plt.imshow(example_face, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the reconstructed face:\n",
    "denoised_face = model(torch.from_numpy(faces.data[0]).float().reshape(-1, 1, 64, 64)).detach().numpy()\n",
    "print(denoised_face.shape)\n",
    "plt.imshow(denoised_face[0,0,...], cmap=\"gray\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## History of Diffusion Models\n",
    "\n",
    "So that's where we find ourselves in the year 2018: If we want to build a diffusion-reversing model, we iteratively apply a U-Net to our noisy image, and we can get out a pretty good denoised reconstruction. But it's SLOW! We have to run a huge U-Net to generate large images. It's time for a key insight: \n",
    "\n",
    "## ✨ The Key Insight ✨\n",
    "\n",
    "**We don't need to run a U-Net to denoise an image. We can just run a U-Net to denoise a latent vector!**\n",
    "\n",
    "This key insight is (among others) the core, critical, transformative idea behind latent diffusion: As far as the neural network is concerned, there's no difference between denoising an image, like we did above, and denoising any other vector. So why waste all our time running a thicc U-Net on an image when we have a perfectly good way of reducing the dimensionality of an image into a latent vector that is much, much smaller?\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/693511/216769571-1d8f3e84-a8f2-4e9b-a57b-3d5933784ea8.png)\n",
    "\n",
    "\n",
    "## Our Grand Architecture\n",
    "\n",
    "We've now implemented nearly all of the model components that go into a Latent Diffusion model: \n",
    "\n",
    "* We have an autoencoder that takes an image and converts it to a latent representation, and the reverse. \n",
    "* We have a U-Net that takes a latent representation WITH noise, and converts it to a latent representation WITHOUT noise, reversing the diffusion process.\n",
    "\n",
    "\n",
    "You'll notice there's a piece of this puzzle missing: we haven't written the \"Text Encoder\" model yet! Luckily, neither have the authors of the stable diffusion paper: there's no reason to re-train a text encoder from scratch, because there are perfectly good pre-trained text encoders out there. So we'll just use one of those. (If this makes you feel like we've cheated, I'd encourage you to try writing a \"text encoder\" to condition the model after this notebook by training a one-hot encoder of image-class on a simpler dataset; it requires a LOT of data to get dependable results for a proper language model, which is why it's out of scope for this notebook. If you still feel cheated out of a cool text encoder, just wait a few weeks... We'll get there!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title How did this section feel?  { display-mode: \"form\" }\n",
    "#@markdown This is a new feedback format we are trying out. You will see these prompts throughout the notebook. Please let us know your candid (anonymous) feedback! This will not be associated with your quiz answers.\n",
    "\n",
    "DatatopsContentReviewContainer(\n",
    "    \"\",\n",
    "    \"W8D2_Grand_Architecture\",\n",
    "    {\n",
    "        \"url\": dt_url,\n",
    "        \"name\": feedback_name,\n",
    "        \"user_key\": feedback_dtid,\n",
    "    }\n",
    ").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentUNet(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A U-net that operates on latents, so it uses linear layers instead of \n",
    "    convolutions, like our previous example did.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_size= 64*64, depth=4):\n",
    "        \"\"\"\n",
    "        A U net that operates on latents, so it uses linear layers instead of convolutions.\n",
    "        \"\"\"\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "        def _down_layer(in_ch, out_ch):\n",
    "            # A sequence of two linear layers with ReLU activation. (If you\n",
    "            # want to use a different architecture, you can change this!)\n",
    "            raise NotImplementedError(\"Implement the `_down_layer` function for the LatentUNet.\")\n",
    "            return nn.Sequential(\n",
    "                ...\n",
    "            )\n",
    "        \n",
    "        def _up_layer(in_ch, out_ch):\n",
    "            # A sequence of two linear layers with ReLU activation. (If you\n",
    "            # want to use a different architecture, you can change this!)\n",
    "            raise NotImplementedError(\"Implement the `_up_layer` function for the LatentUNet.\")\n",
    "            return nn.Sequential(\n",
    "                ...\n",
    "            )\n",
    "\n",
    "        super(LatentUNet, self).__init__()\n",
    "\n",
    "        # We'll do the hard part for you and implement the nasty U-net \n",
    "        # concatenation logic!\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        v = vector_size\n",
    "        for i in range(depth):\n",
    "            self.downs.append(_down_layer(v, v//2))\n",
    "            v = v//2\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.ups.append(_up_layer(v, v*2))\n",
    "            v = v*2\n",
    "\n",
    "        self.last_layer = nn.Linear(v, vector_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.vector_size)\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "        for up in self.ups:\n",
    "            x = up(x)\n",
    "        output = self.last_layer(x)\n",
    "        return output.reshape(-1, self.vector_size)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, x.reshape(-1, 64*64))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few new faces from scratch, and show that we can achieve \n",
    "# not-totally-awful results on our latent diffusion model that we trained in\n",
    "# only a few minutes.\n",
    "\n",
    "class LatentDiffusionToy(pl.LightningModule):\n",
    "    def __init__(self, embedder, denoiser, lr=1e-3):\n",
    "        super(LatentDiffusionToy, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.denoiser = denoiser\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the image:\n",
    "        x = x.view(-1, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "        latent = self.embedder.encoder(x)\n",
    "        # Denoise the latent:\n",
    "        denoised_latent = self.denoiser(latent)\n",
    "        # Decode the denoised latent:\n",
    "        return self.embedder.decoder(denoised_latent)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, x.view(-1, IMG_CHANNELS, IMG_SIZE, IMG_SIZE))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, latent=None):\n",
    "        if latent is None:\n",
    "            latent = torch.randn(1, 256)\n",
    "        assert latent.shape[1:] == (256,)\n",
    "        return self.embedder.decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out model's pieces:\n",
    "denoiser = LatentUNet(256)\n",
    "# Image-to-latent net:\n",
    "embedder = autoencoder # make sure you ran the cells above :)\n",
    "\n",
    "# Train it!\n",
    "model = LatentDiffusionToy(embedder, denoiser)\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\", gpus=1)\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train, we read an image from the training set, and then run it through the ENCODER of the AE to get a latent. We then run it through the U-net to denoise (for time's sake, we only run this through once, but in \"real life\" this is done iteratively MANY times!). Finally, we take the denoised latent and run it through the decoder to get an image.\n",
    "\n",
    "When we SAMPLE (see below), we take a random vector that is the right size for the latent space, and we run it through the U-net to denoise it. Then we run it through the decoder to get an image. No source image, but we still get a pretty good image out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "for i in range(3):\n",
    "    axs[i].imshow(model.sample().detach().numpy()[0, 0, ...], cmap=\"gray\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz time! \n",
    "\n",
    "GANs suffer from mode collapse (when the generator learns a mapping from the latent space to the output space that collapses multiple input points to a single output point, resulting in the generator producing only a small set of outputs). Why doesn't LD suffer from the same problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_no_mode_collapse = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we didn't experience mode collapse, all of the faces we generated seemed kinda similar. Why do you think our model consistently outputs similar-looking faces? What could we do to our architecture to increase the degrees of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "why_similar_faces = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a LLM (e.g., https://chat.openai.com/chat) to explain what stable diffusion is, in simple terms. What does it get right? What does it get wrong? Were you able to use a LLM such as Copilot to write any of the models in this notebook? Why do you think it fails or succeeds at the tasks you gave it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_does_llm_say = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your quiz answers (run this cell to submit)\n",
    "\n",
    "quizdt.store(\n",
    "    dict(\n",
    "        why_pca_isnt_a_good_denoiser=why_pca_isnt_a_good_denoiser,\n",
    "        why_no_mode_collapse=why_no_mode_collapse,\n",
    "        why_similar_faces=why_similar_faces,\n",
    "        what_does_llm_say=what_does_llm_say,\n",
    "        my_pennkey=my_pennkey,\n",
    "        my_pod=my_pod,\n",
    "        my_email=my_email,\n",
    "    )\n",
    ")\n",
    "print(\"Submitted!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b81b370e9c0dc1906fe4c8135ff0ada4d70e528519389c4f900e14566b83adf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
